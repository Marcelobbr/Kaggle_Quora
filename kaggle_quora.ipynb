{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - quora challenge\n",
    "Team: marsamag\n",
    "* Marcelo Barata Ribeiro\n",
    "* Magno Mendes \n",
    "* Sayuri Takeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from numpy import float32\n",
    "import scipy\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "import pandas\n",
    "import string\n",
    "import warnings\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a small sample to test if every function is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train_all = pandas.read_csv(\"/Dados/Kaggle/train.csv\")\n",
    "#quora_train = quora_train_all.head(105785) # data reduction for tests with sample\n",
    "#quora_test = pandas.read_csv(\"/Dados/Kaggle/test.csv\")\n",
    "#df_test = quora_test.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = quora_train_all[1:3] # data reduction for tests with sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repair errors in the database: missing phrases that became nan on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repair_original_data(data):\n",
    "    for row in data.itertuples():\n",
    "        for question in ['question1', 'question2']:\n",
    "            try: len(data.ix[row[0], question])\n",
    "            except: data.ix[row[0], question] = \"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   id  qid1  qid2                                          question1  \\\n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2   2     5     6  How can I increase the speed of my internet co...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  How can Internet speed be increased by hacking...             0  \n"
     ]
    }
   ],
   "source": [
    "print (type(quora_train))\n",
    "print(quora_train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAEJCAYAAACT0Y7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsgcAwMtR9gAAeLn/B6Uk6Zjd0A3HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54038bdc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = sns.color_palette()\n",
    "\n",
    "is_dup = quora_train['is_duplicate'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "sns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=color[1])\n",
    "\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "\n",
    "plt.xlabel('Is Duplicate', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create functions to process data\n",
    "We used some functions made by Guilherme Wang and Lucas Medeiros team, because its functions were well writen compared to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Editing questions with NLTK package\n",
    "\n",
    "def remove_stopwords(phrase,list_stopwords):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all stopwords from a list\n",
    "    :param phrase: String. A phrase.\n",
    "    :param list_stopwords: List. A list of stopwords\n",
    "    :return: The same phrase without stopwords\n",
    "    \"\"\"\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in list_stopwords:\n",
    "            final_phrase.append((word))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "    \n",
    "def remove_punctuation(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all punctuation from it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase without punctuation\n",
    "    \"\"\"\n",
    "    #Check if NA\n",
    "    if type(phrase) is float:\n",
    "        if math.isnan(phrase):\n",
    "            return (\"\")\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    phrase = phrase.translate(translator) #removing punctuation\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "def lemm_wordnet(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes lemmatizes it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lemmas\n",
    "    \"\"\"\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    #NA is a float type, so this if is to avoid conflict\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    else:\n",
    "        return \"\"\n",
    "    return phrase\n",
    "    \n",
    "def remove_duplicate(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all duplicate words\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase with just unique words\n",
    "    \"\"\"\n",
    "    aux_phrase = []\n",
    "        \n",
    "    if type(phrase) is not float:\n",
    "        \n",
    "        for i in phrase.split():\n",
    "            \n",
    "            if i not in aux_phrase:\n",
    "                aux_phrase.append(i)\n",
    "    \n",
    "    phrase = ' '.join(aux_phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "    \n",
    "def all_lower_case(phrase):    \n",
    "    \"\"\"\n",
    "    Receives a phrase and makes it lower case\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lower case\n",
    "    \"\"\"\n",
    "    if type(phrase) is not float:\n",
    "            phrase = phrase.lower()\n",
    "    return phrase\n",
    "    \n",
    "def stem_snowball(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and returns the same phrase stemmed, lowercase phrase without stopwords\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: String. Stemmed, lowercase phrase without stopwords\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #Stem words according to stemmer\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        final_phrase.append((stemmer.stem(word)))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning tool is used so you can easily choose which functions you want to use to clean te text\n",
    "def cleaning_tool(data, drop_na = True, lower_case = True, rm_duplicate = True, stopwords = True, \n",
    "                  punctuation = True, lemm = False, stem = False):\n",
    "    \"\"\"\n",
    "    Function to process all data using calling functions from above, according to what was chosen.\n",
    "    :param data: data frame.\n",
    "    :param drop_na: If True drop all lines of data frame with NA\n",
    "    :param lower_case: If True transform for lower case\n",
    "    :param rm_duplicate: If True remove all duplicate words in questions\n",
    "    :param stopwords: If True removes stopwords\n",
    "    :param punctuation: If True removes punctuation\n",
    "    :param lemm: If True returns the phrase lemmatized\n",
    "    :param stem: If True returns the phrase stemmed\n",
    "    :param list_of_stopwords: List of stopwords to be used\n",
    "    :return: Question1 and Question2 processed according to parameters\n",
    "    \"\"\"\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)\n",
    "    \n",
    "    if rm_duplicate == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_duplicate(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_duplicate(x))\n",
    "    \n",
    "    if lower_case == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: all_lower_case(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: all_lower_case(x))\n",
    "    \n",
    "    if stopwords == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_stopwords(x, list_stopwords))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_stopwords(x, list_stopwords))\n",
    "       \n",
    "    if punctuation == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_punctuation(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_punctuation(x))\n",
    "        \n",
    "    if lemm_wordnet == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: lemm_wordnet(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: lemm_wordnet(x))\n",
    "        \n",
    "    if stem_snowball == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: stem_snowball(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: stem_snowball(x))\n",
    "    \n",
    "    #We used it two times if some function create a new NA.\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create length and fuzzywuzzy functions\n",
    "Here we create some features for the data, such as phrase length, word length and fuzzyWuzzy features which use Levenstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_basic_features(data):\n",
    "    data[\"len_q1\"] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data[\"len_q2\"] = data.question2.apply(lambda x: len(str(x)))\n",
    "    data[\"diff_len\"] = data.len_q1 - data.len_q2\n",
    "    data[\"len_char_q1\"] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data[\"len_char_q2\"] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data[\"len_word_q1\"] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data[\"len_word_q2\"] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "    data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    return data\n",
    "\n",
    "def make_fuzz_features(data):\n",
    "    data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_partial_token _set_ratio'] = data.apply(lambda x : fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz. partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data ['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec  (wikimedia model)\n",
    "Here we create word2vec features and some distance features, such as cossine distance, euclidean distance and Cabberra distance.\n",
    "\n",
    "But we don't use those features for the submission, because it was too slow to process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_io_files = '/Dados/Word2vec/'\n",
    "wikimedia = os.path.join(path_io_files,'model_wikimedia_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_wikimedia = Word2Vec.load(wikimedia)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "googlenews = os.path.join(path_io_files,'GoogleNews-vectors-negative300.bin.gz')\n",
    "model_googlenews = gensim.models.KeyedVectors.load_word2vec_format(googlenews, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 400    # Word vector dimensionality\n",
    "\n",
    "def makeFeatureVec(words, model,index2word_set, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = numpy.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words.split(\" \"):\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = numpy.add(featureVec,model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = numpy.divide(featureVec,nwords)\n",
    "    #    if all(math.isnan(i) == True\" for i in featureVec): featureVec = numpy.zeros((num_features,),dtype=\"float32\")\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create functions of vector and distance features from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_vectors(df1, df2):\n",
    "    df3 = numpy.add(df1,df2)/2\n",
    "    df_final = numpy.column_stack((df3))\n",
    "    return (df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word_set = set(model_wikimedia.wv.index2word)\n",
    "\n",
    "def make_vector_features(data, question1_vectors, question2_vectors):\n",
    "    #print(type(data))\n",
    "    #question1_vectors = []\n",
    "    #question2_vectors = []\n",
    "    for index in range(0,len(data)):\n",
    "        question1_vectors.append(makeFeatureVec(data[\"question1\"].iloc[index],model_wikimedia,index2word_set,num_features))\n",
    "        question2_vectors.append(makeFeatureVec(data[\"question2\"].iloc[index],model_wikimedia,index2word_set,num_features))\n",
    "    question1_vectors = numpy.array(question1_vectors)\n",
    "    question2_vectors = numpy.array(question2_vectors)\n",
    "    \n",
    "    #NANs to 0\n",
    "    where_are_NaNs_1 = numpy.isnan(question1_vectors)\n",
    "    where_are_NaNs_2 = numpy.isnan(question2_vectors)\n",
    "    question1_vectors[where_are_NaNs_1] = 0\n",
    "    question2_vectors[where_are_NaNs_2] = 0\n",
    "    #nan2num_1 = numpy.nan_to_num(question1)\n",
    "    #nan2num_2 = numpy.nan_to_num(question2)\n",
    "    \n",
    "    count = 0\n",
    "    empty_array = numpy.empty([1000, 1])\n",
    "        \n",
    "    features_wikimedia = (add_vectors(question1_vectors,question2_vectors))\n",
    "    #features_wikimedia2 = numpy.transpose(features_wikimedia)\n",
    "    \n",
    "    wikimedia_df = pandas.DataFrame(features_wikimedia)\n",
    "    wikimedia_df = wikimedia_df.transpose()\n",
    "\n",
    "    \n",
    "    #variables_list = list(wikimedia_df.columns.values)\n",
    "    #print(variables_list)\n",
    "    #print(type(data))\n",
    "    \n",
    "    data_id = data['id']\n",
    "    data_id = pandas.DataFrame(data_id)\n",
    "    print(data_id)\n",
    "    \n",
    "    wikimedia_df['id'] = data_id['id']\n",
    "    print(wikimedia_df)\n",
    "    wikimedia_df = wikimedia_df.set_index(['id'])\n",
    "    print(wikimedia_df)\n",
    "    #for column in wikimedia_df:\n",
    "    #    print(column)\n",
    "    #    data[column] = wikimedia_df[column]\n",
    "    #    break\n",
    "    data = data.join(wikimedia_df, lsuffix='_left', rsuffix='_right')\n",
    "    \n",
    "    return data\n",
    "    return question1_vectors\n",
    "    return question2_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first try to fix indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = quora_train_all[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = []\n",
    "v2 = []\n",
    "quora_train = make_vector_features(quora_train, v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_id = quora_train['id']\n",
    "data_id = pandas.DataFrame(data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in quora_train:\n",
    "    data_id[column] = quora_train[column]\n",
    "data_id['test'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = quora_train.merge(data_id, on = 'id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  test  \n",
       "1  What would happen if the Indian government sto...             0     2  \n",
       "2  How can Internet speed be increased by hacking...             0     2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_list = []\n",
    "for n in quora_train['id']:\n",
    "    ids_list.append(n)\n",
    "print(ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dist_features(data):\n",
    "    data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create deletion and error checking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to delete features that are irrelevant for the machine learning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_features(data):\n",
    "    del data['question1']\n",
    "    del data['question2']\n",
    "    try: \n",
    "        del data['qid1']\n",
    "        del data['qid2']\n",
    "    except: pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to repair NaNs and check existing variables\n",
    "def repair_cells(data):\n",
    "    variables_list = list(quora_train.columns.values)\n",
    "    print('LIST OF VARIABLES:')\n",
    "    print(variables_list)\n",
    "    for var in variables_list: quora_train[var].fillna(quora_train[var].dropna().median(), inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = quora_train\n",
    "df_test = quora_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_qs = pandas.Series(quora_train['question1'].tolist() + quora_train['question2'].tolist()).astype(str)\n",
    "test_qs = pandas.Series(quora_test['question1'].tolist() + quora_test['question2'].tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = numpy.sum(shared_weights) / numpy.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebalancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "train_word_match = quora_train.apply(word_match_share, axis=1, raw=True)\n",
    "tfidf_train_word_match = quora_train.apply(tfidf_word_match_share, axis=1, raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404285    0.857143\n",
       "404286    0.666667\n",
       "404287    0.500000\n",
       "404288    0.000000\n",
       "404289    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_match.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# First we create our training and testing data\n",
    "x_train = pandas.DataFrame()\n",
    "x_test = pandas.DataFrame()\n",
    "x_train['word_match'] = train_word_match\n",
    "x_train['tfidf_word_match'] = tfidf_train_word_match\n",
    "x_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n",
    "x_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "y_train = quora_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "\n",
    "# Oversampling the negative class\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pandas.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pandas.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train = pandas.concat([pos_train, neg_train])\n",
    "y_train = (numpy.zeros(len(pos_train)) + 1).tolist() + numpy.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202450</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175581</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.110683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261865</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94259</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.187747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271928</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.463781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_match  tfidf_word_match\n",
       "202450    0.571429          0.571005\n",
       "175581    0.100000          0.110683\n",
       "261865    0.363636          0.244444\n",
       "94259     0.142857          0.187747\n",
       "271928    0.444444          0.463781"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.683189\tvalid-logloss:0.683238\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.602041\tvalid-logloss:0.602515\n",
      "[20]\ttrain-logloss:0.544863\tvalid-logloss:0.545663\n",
      "[30]\ttrain-logloss:0.503151\tvalid-logloss:0.504194\n",
      "[40]\ttrain-logloss:0.471989\tvalid-logloss:0.473231\n",
      "[50]\ttrain-logloss:0.448341\tvalid-logloss:0.449746\n",
      "[60]\ttrain-logloss:0.430164\tvalid-logloss:0.431706\n",
      "[70]\ttrain-logloss:0.416072\tvalid-logloss:0.417726\n",
      "[80]\ttrain-logloss:0.405012\tvalid-logloss:0.406775\n",
      "[90]\ttrain-logloss:0.396327\tvalid-logloss:0.398177\n",
      "[100]\ttrain-logloss:0.389488\tvalid-logloss:0.391404\n",
      "[110]\ttrain-logloss:0.384071\tvalid-logloss:0.386038\n",
      "[120]\ttrain-logloss:0.379786\tvalid-logloss:0.381792\n",
      "[130]\ttrain-logloss:0.376375\tvalid-logloss:0.378414\n",
      "[140]\ttrain-logloss:0.373661\tvalid-logloss:0.375724\n",
      "[150]\ttrain-logloss:0.371482\tvalid-logloss:0.373567\n",
      "[160]\ttrain-logloss:0.369728\tvalid-logloss:0.371835\n",
      "[170]\ttrain-logloss:0.368327\tvalid-logloss:0.370453\n",
      "[180]\ttrain-logloss:0.367186\tvalid-logloss:0.369324\n",
      "[190]\ttrain-logloss:0.366276\tvalid-logloss:0.368424\n",
      "[200]\ttrain-logloss:0.365539\tvalid-logloss:0.367697\n",
      "[210]\ttrain-logloss:0.364945\tvalid-logloss:0.367108\n",
      "[220]\ttrain-logloss:0.364443\tvalid-logloss:0.366615\n",
      "[230]\ttrain-logloss:0.364033\tvalid-logloss:0.366215\n",
      "[240]\ttrain-logloss:0.363691\tvalid-logloss:0.365883\n",
      "[250]\ttrain-logloss:0.363389\tvalid-logloss:0.3656\n",
      "[260]\ttrain-logloss:0.363117\tvalid-logloss:0.365335\n",
      "[270]\ttrain-logloss:0.362879\tvalid-logloss:0.365108\n",
      "[280]\ttrain-logloss:0.362655\tvalid-logloss:0.364893\n",
      "[290]\ttrain-logloss:0.362455\tvalid-logloss:0.364698\n",
      "[300]\ttrain-logloss:0.362288\tvalid-logloss:0.36454\n",
      "[310]\ttrain-logloss:0.362151\tvalid-logloss:0.364412\n",
      "[320]\ttrain-logloss:0.362029\tvalid-logloss:0.364298\n",
      "[330]\ttrain-logloss:0.361896\tvalid-logloss:0.364174\n",
      "[340]\ttrain-logloss:0.361792\tvalid-logloss:0.364075\n",
      "[350]\ttrain-logloss:0.361691\tvalid-logloss:0.363984\n",
      "[360]\ttrain-logloss:0.361568\tvalid-logloss:0.36387\n",
      "[370]\ttrain-logloss:0.361455\tvalid-logloss:0.363768\n",
      "[380]\ttrain-logloss:0.361353\tvalid-logloss:0.363673\n",
      "[390]\ttrain-logloss:0.361258\tvalid-logloss:0.363586\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pandas.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Length features, Fuzzy features, wordvec + other ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>5939</td>\n",
       "      <td>5940</td>\n",
       "      <td>Are chocolates and cocaine (drug) made from sa...</td>\n",
       "      <td>Is mocha made with milk or dark chocolate?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>5941</td>\n",
       "      <td>5942</td>\n",
       "      <td>What is 3D builder in windows 10?</td>\n",
       "      <td>Do you really need an antivirus software for W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>5943</td>\n",
       "      <td>5944</td>\n",
       "      <td>What is the gear ratio when the car is being d...</td>\n",
       "      <td>What is gear ratio?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>5945</td>\n",
       "      <td>5946</td>\n",
       "      <td>Where can I get Etoos video lectures at a cybe...</td>\n",
       "      <td>Where can I get Etoos video lectures at a cybe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>5947</td>\n",
       "      <td>5948</td>\n",
       "      <td>How do I hide my IP address?</td>\n",
       "      <td>Is Tor basically the same as software that hid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  qid1  qid2                                          question1  \\\n",
       "2995  2995  5939  5940  Are chocolates and cocaine (drug) made from sa...   \n",
       "2996  2996  5941  5942                  What is 3D builder in windows 10?   \n",
       "2997  2997  5943  5944  What is the gear ratio when the car is being d...   \n",
       "2998  2998  5945  5946  Where can I get Etoos video lectures at a cybe...   \n",
       "2999  2999  5947  5948                       How do I hide my IP address?   \n",
       "\n",
       "                                              question2  is_duplicate  \n",
       "2995         Is mocha made with milk or dark chocolate?             0  \n",
       "2996  Do you really need an antivirus software for W...             0  \n",
       "2997                                What is gear ratio?             0  \n",
       "2998  Where can I get Etoos video lectures at a cybe...             0  \n",
       "2999  Is Tor basically the same as software that hid...             0  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = repair_original_data(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean text (example: remove stopwords)\n",
    "quora_train = cleaning_tool(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = make_basic_features(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = make_fuzz_features(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "vectors1 = []\n",
    "vectors2 = []\n",
    "quora_train = make_vector_features(quora_train, vectors1, vectors2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# not effective, commented\n",
    "quora_train = make_dist_features(quora_train)\n",
    "\n",
    "#check error (solved): ValueError: ('array must not contain infs or NaNs', 'occurred at index 292')\n",
    "#also 54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete text and id features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = delete_features(quora_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solving errors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# solve errors on rows (obsolete, commented)\n",
    "quora_train = repair_cells(quora_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# error checker on rows (obsolete, commented)\n",
    "\n",
    "quora_train = quora_train.apply(lambda s: s[numpy.isfinite(s)].dropna()).sum()\n",
    "\n",
    "errorshow = 'No'\n",
    "for row in range(1000):\n",
    "    for value in quora_train.iloc[row]:\n",
    "        if math.isnan(value) == True:\n",
    "            print('nan', value, type(value))\n",
    "            errorshow = 'Yes'\n",
    "        if not math.isfinite(value) == True:\n",
    "            print('isfinite', value, type(value))\n",
    "            errorshow = 'Yes'\n",
    "    if errorshow == 'Yes': print(quora_train.iloc[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory rescaling\n",
    "probable fix to too large integers for dtype('float32') on random forest model (obsolete, commented)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "quora_train = quora_train.astype(float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.060841</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>0.027746</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>-0.014814</td>\n",
       "      <td>-0.027185</td>\n",
       "      <td>-0.023458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>57</td>\n",
       "      <td>-36</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019601</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>-0.023130</td>\n",
       "      <td>0.036786</td>\n",
       "      <td>-0.005226</td>\n",
       "      <td>-0.019917</td>\n",
       "      <td>0.028215</td>\n",
       "      <td>0.038793</td>\n",
       "      <td>-0.021435</td>\n",
       "      <td>0.040055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.081929</td>\n",
       "      <td>-0.040642</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>0.044221</td>\n",
       "      <td>-0.003188</td>\n",
       "      <td>-0.007723</td>\n",
       "      <td>0.034219</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.024946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020739</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>-0.032050</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.020132</td>\n",
       "      <td>-0.009288</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>-0.019179</td>\n",
       "      <td>0.012429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>-30</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.033650</td>\n",
       "      <td>-0.048778</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>0.006210</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>-0.023818</td>\n",
       "      <td>0.021476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  417 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  is_duplicate  len_q1  len_q2  diff_len  len_char_q1  len_char_q2  \\\n",
       "2995  2995             0      35      30         5           16           12   \n",
       "2996  2996             0      21      57       -36           14           19   \n",
       "2997  2997             0      34      10        24           13            7   \n",
       "2998  2998             0      42      41         1           18           17   \n",
       "2999  2999             0      15      45       -30            8           17   \n",
       "\n",
       "      len_word_q1  len_word_q2  common_words    ...          390       391  \\\n",
       "2995            5            5             1    ...     0.000686 -0.000891   \n",
       "2996            4            8             2    ...    -0.019601  0.009487   \n",
       "2997            6            2             2    ...    -0.000033  0.081929   \n",
       "2998            7            7             6    ...    -0.020739  0.019508   \n",
       "2999            3            7             2    ...     0.025421  0.033650   \n",
       "\n",
       "           392       393       394       395       396       397       398  \\\n",
       "2995 -0.060841  0.001139 -0.001996  0.027746  0.009978 -0.014814 -0.027185   \n",
       "2996 -0.023130  0.036786 -0.005226 -0.019917  0.028215  0.038793 -0.021435   \n",
       "2997 -0.040642  0.006309  0.044221 -0.003188 -0.007723  0.034219  0.015010   \n",
       "2998 -0.032050  0.001310  0.011551  0.020132 -0.009288  0.011333 -0.019179   \n",
       "2999 -0.048778 -0.002515  0.006210  0.013893 -0.033194  0.001398 -0.023818   \n",
       "\n",
       "           399  \n",
       "2995 -0.023458  \n",
       "2996  0.040055  \n",
       "2997  0.024946  \n",
       "2998  0.012429  \n",
       "2999  0.021476  \n",
       "\n",
       "[5 rows x 417 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_quora_train = quora_train.drop(\"is_duplicate\", axis=1)\n",
    "y_quora_train = quora_train[\"is_duplicate\"]\n",
    "quora_train_features, quora_test_features, quora_train_y, quora_test_y = model_selection.train_test_split(\n",
    "    x_quora_train, y_quora_train, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier(n_estimators=300, max_features='auto', bootstrap=False, \n",
    "                               oob_score=False, n_jobs=-1, random_state=0)\n",
    "randomforest.fit(quora_train_features, quora_train_y)\n",
    "\n",
    "predict = randomforest.predict_proba(quora_test_features)\n",
    "\n",
    "logloss_random_forest = log_loss(quora_test_y,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49342453558698612"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length must be 30% of used sample\n",
    "len(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**knn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(quora_train_features, quora_train_y)\n",
    "\n",
    "knn_predict = knn.predict_proba(quora_test_features)\n",
    "\n",
    "logloss_knn = log_loss(quora_test_y,knn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adaboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aboost = AdaBoostClassifier(base_estimator=None,\n",
    "                             n_estimators=200,\n",
    "                             learning_rate=0.1,\n",
    "                             algorithm='SAMME.R',\n",
    "                             random_state=0).fit(X_traincv, y_traincv)\n",
    "aboost.fit(quora_train_features, quora_train_y)\n",
    "\n",
    "aboost_predict = aboost.predict_proba(quora_test_features)\n",
    "\n",
    "logloss_aboost = log_loss(quora_test_y,aboost_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>0.602758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>5.179770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model     Score\n",
       "0  randomforest  0.602758\n",
       "1           knn  5.179770"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pandas.DataFrame({\n",
    "    'Model': ['randomforest', 'knn', 'aboost'],\n",
    "    'Score': [logloss_random_forest, logloss_knn, logloss_aboost]})\n",
    "models.sort_values(by='Log Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52000000000000002,\n",
       " 0.28666666666666668,\n",
       " 0.69666666666666666,\n",
       " 0.11,\n",
       " 0.49333333333333335]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_submission = [i[1] for i in predict]\n",
    "pred_submission[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighting predictions\n",
    "Forum discussions showed that the disctibution is imbalaced, with only 20% of the pairs being duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37689944444444451"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mean=numpy.mean(pred_submission)\n",
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_pred_submission = []\n",
    "for pred in pred_submission:\n",
    "    weighted_pred=pred*(0.2/pred_mean)\n",
    "    weighted_pred_submission.append(weighted_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(weighted_pred_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xgboost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(quora_train_features, label=quora_train_y)\n",
    "d_valid = xgb.DMatrix(quora_test_features, label=quora_test_y)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d_test = xgb.DMatrix(quora_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pandas.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.682157\tvalid-logloss:0.68321\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.59224\tvalid-logloss:0.603063\n",
      "[20]\ttrain-logloss:0.527537\tvalid-logloss:0.548144\n",
      "[30]\ttrain-logloss:0.479277\tvalid-logloss:0.508679\n",
      "[40]\ttrain-logloss:0.442521\tvalid-logloss:0.479956\n",
      "[50]\ttrain-logloss:0.414195\tvalid-logloss:0.458563\n",
      "[60]\ttrain-logloss:0.392105\tvalid-logloss:0.442582\n",
      "[70]\ttrain-logloss:0.37487\tvalid-logloss:0.430409\n",
      "[80]\ttrain-logloss:0.360245\tvalid-logloss:0.422135\n",
      "[90]\ttrain-logloss:0.348409\tvalid-logloss:0.416162\n",
      "[100]\ttrain-logloss:0.338818\tvalid-logloss:0.411884\n",
      "[110]\ttrain-logloss:0.331215\tvalid-logloss:0.408584\n",
      "[120]\ttrain-logloss:0.324763\tvalid-logloss:0.406395\n",
      "[130]\ttrain-logloss:0.318969\tvalid-logloss:0.405296\n",
      "[140]\ttrain-logloss:0.313625\tvalid-logloss:0.404541\n",
      "[150]\ttrain-logloss:0.308733\tvalid-logloss:0.403617\n",
      "[160]\ttrain-logloss:0.304384\tvalid-logloss:0.403511\n",
      "[170]\ttrain-logloss:0.300362\tvalid-logloss:0.40271\n",
      "[180]\ttrain-logloss:0.296649\tvalid-logloss:0.402225\n",
      "[190]\ttrain-logloss:0.293246\tvalid-logloss:0.402164\n",
      "[200]\ttrain-logloss:0.290306\tvalid-logloss:0.401498\n",
      "[210]\ttrain-logloss:0.287539\tvalid-logloss:0.401394\n",
      "[220]\ttrain-logloss:0.285011\tvalid-logloss:0.401194\n",
      "[230]\ttrain-logloss:0.282829\tvalid-logloss:0.401465\n",
      "[240]\ttrain-logloss:0.280763\tvalid-logloss:0.401426\n",
      "[250]\ttrain-logloss:0.278768\tvalid-logloss:0.40146\n",
      "[260]\ttrain-logloss:0.276644\tvalid-logloss:0.401186\n",
      "[270]\ttrain-logloss:0.27511\tvalid-logloss:0.400676\n",
      "[280]\ttrain-logloss:0.273851\tvalid-logloss:0.400163\n",
      "[290]\ttrain-logloss:0.272394\tvalid-logloss:0.399763\n",
      "[300]\ttrain-logloss:0.270977\tvalid-logloss:0.399689\n",
      "[310]\ttrain-logloss:0.26975\tvalid-logloss:0.3994\n",
      "[320]\ttrain-logloss:0.268451\tvalid-logloss:0.399323\n",
      "[330]\ttrain-logloss:0.267018\tvalid-logloss:0.400093\n",
      "[340]\ttrain-logloss:0.265668\tvalid-logloss:0.400681\n",
      "[350]\ttrain-logloss:0.263752\tvalid-logloss:0.40096\n",
      "[360]\ttrain-logloss:0.261294\tvalid-logloss:0.401048\n",
      "Stopping. Best iteration:\n",
      "[319]\ttrain-logloss:0.268589\tvalid-logloss:0.399294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(quora_train_features, quora_train_y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pandas.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Here we submit our predictions using all the training and testing databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = pandas.read_csv(\"/Dados/Kaggle/train.csv\")\n",
    "quora_test = pandas.read_csv(\"/Dados/Kaggle/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_selected_functions(quora_data):\n",
    "    quora_data = repair_original_data(quora_data)\n",
    "    #quora_data = cleaning_tool(quora_data)\n",
    "    #quora_data = make_basic_features(quora_data)\n",
    "    #quora_data = make_fuzz_features(quora_data)\n",
    "    #vectors1 = []\n",
    "    #vectors2 = []\n",
    "    #quora_data = make_vector_features(quora_data, vectors1, vectors2)\n",
    "    #quora_data = delete_features(quora_data)\n",
    "    return quora_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "quora_train = use_selected_functions(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_test = use_selected_functions(quora_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for number in quora_train_model['id']:\n",
    "    if number != count: \n",
    "        print(number)\n",
    "        count += 1\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrector = quora_train[105780:105786]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in corrector.itertuples():\n",
    "    print(row[0])\n",
    "    #corrector.loc[row,'question2'] = 2\n",
    "    try: print(len(corrector.ix[row[0], 'question2']))\n",
    "    except: corrector.ix[row[0], 'question2'] = \"\"\n",
    "    #if corrector.ix[row[0], 'question2'].isnan:\n",
    "    #    print('nan')\n",
    "    #    corrector.ix[row[0], 'question2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use  instead of ?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>Method to find separation of slits using fresn...</td>\n",
       "      <td>What are some of the things technicians can te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>What are the laws to change your status from a...</td>\n",
       "      <td>What are the laws to change your status from a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>Why do girls want to be friends with the guy t...</td>\n",
       "      <td>How do guys feel after rejecting a girl?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>Why are so many Quora users posting questions ...</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>Which is the best digital marketing institutio...</td>\n",
       "      <td>Which is the best digital marketing institute ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>What's causing someone to be jealous?</td>\n",
       "      <td>What can I do to avoid being jealous of someone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>What are the questions should not ask on Quora?</td>\n",
       "      <td>Which question should I ask on Quora?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>How much is 30 kV in HP?</td>\n",
       "      <td>Where can I find a conversion chart for CC to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>What does it mean that every time I look at th...</td>\n",
       "      <td>How many times a day do a clocks hands overlap?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>What are some tips on making it through the jo...</td>\n",
       "      <td>What are some tips on making it through the jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>What is web application?</td>\n",
       "      <td>What is the web application framework?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>Does society place too much importance on sports?</td>\n",
       "      <td>How do sports contribute to the society?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>What is best way to make money online?</td>\n",
       "      <td>What is best way to ask for money online?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>How should I prepare for CA final law?</td>\n",
       "      <td>How one should know that he/she completely pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105755</th>\n",
       "      <td>105755</td>\n",
       "      <td>174332</td>\n",
       "      <td>174333</td>\n",
       "      <td>Should aspiring billionaires be as multifacete...</td>\n",
       "      <td>Can you tell that someone has been looking at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105756</th>\n",
       "      <td>105756</td>\n",
       "      <td>174334</td>\n",
       "      <td>68791</td>\n",
       "      <td>What is the purpose of a board of directors?</td>\n",
       "      <td>What are the responsibilities of a board of di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105757</th>\n",
       "      <td>105757</td>\n",
       "      <td>174335</td>\n",
       "      <td>84291</td>\n",
       "      <td>Is now a good time to invest in the Melbourne ...</td>\n",
       "      <td>Is now a good time to invest in the California...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105758</th>\n",
       "      <td>105758</td>\n",
       "      <td>174336</td>\n",
       "      <td>174337</td>\n",
       "      <td>Why do I like violence?</td>\n",
       "      <td>Why is there violence in the world?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105759</th>\n",
       "      <td>105759</td>\n",
       "      <td>174338</td>\n",
       "      <td>174339</td>\n",
       "      <td>What is your opinion about pair programming?</td>\n",
       "      <td>What do you think of pair programming?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105760</th>\n",
       "      <td>105760</td>\n",
       "      <td>174340</td>\n",
       "      <td>158724</td>\n",
       "      <td>Why are oceans blue?</td>\n",
       "      <td>Why is the sea blue?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105761</th>\n",
       "      <td>105761</td>\n",
       "      <td>174341</td>\n",
       "      <td>174342</td>\n",
       "      <td>Which torrent allows downloading paid ebooks f...</td>\n",
       "      <td>Does Google pay when your app is downloaded?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105762</th>\n",
       "      <td>105762</td>\n",
       "      <td>174343</td>\n",
       "      <td>174344</td>\n",
       "      <td>How can I get money for free?</td>\n",
       "      <td>How do I get free money?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105763</th>\n",
       "      <td>105763</td>\n",
       "      <td>174345</td>\n",
       "      <td>174346</td>\n",
       "      <td>Which country is best to study in: Canada or A...</td>\n",
       "      <td>Which country holds the best career opportunit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105764</th>\n",
       "      <td>105764</td>\n",
       "      <td>106802</td>\n",
       "      <td>174347</td>\n",
       "      <td>Why would parents abandon their children?</td>\n",
       "      <td>Why do children disobey their parents?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105765</th>\n",
       "      <td>105765</td>\n",
       "      <td>47020</td>\n",
       "      <td>42108</td>\n",
       "      <td>Why do people write questions on Quora that co...</td>\n",
       "      <td>Why do people use Quora when they could easily...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105766</th>\n",
       "      <td>105766</td>\n",
       "      <td>24372</td>\n",
       "      <td>174348</td>\n",
       "      <td>How do I get a Starbucks gold card and what do...</td>\n",
       "      <td>Does a US Starbucks Card work in other countries?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105767</th>\n",
       "      <td>105767</td>\n",
       "      <td>174349</td>\n",
       "      <td>79077</td>\n",
       "      <td>Is majoring in economics a good choice?</td>\n",
       "      <td>Is graduation in economics a good choice?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105768</th>\n",
       "      <td>105768</td>\n",
       "      <td>174350</td>\n",
       "      <td>174351</td>\n",
       "      <td>What are the differences between the Shaytan o...</td>\n",
       "      <td>How do the Muslim beliefs about Shaitan parall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105769</th>\n",
       "      <td>105769</td>\n",
       "      <td>174352</td>\n",
       "      <td>174353</td>\n",
       "      <td>What is it like to be a professional flutist?</td>\n",
       "      <td>Where can I sell windows product keys?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105770</th>\n",
       "      <td>105770</td>\n",
       "      <td>174354</td>\n",
       "      <td>174355</td>\n",
       "      <td>What does the U.S. Navy's motto \"semper fortis...</td>\n",
       "      <td>What is Navy Boot Camp like?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105771</th>\n",
       "      <td>105771</td>\n",
       "      <td>173453</td>\n",
       "      <td>174356</td>\n",
       "      <td>What was Barack Obama's GPA in high school and...</td>\n",
       "      <td>What GPA did Bill Gates get in high school?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105772</th>\n",
       "      <td>105772</td>\n",
       "      <td>174357</td>\n",
       "      <td>174358</td>\n",
       "      <td>When Vince McMahon passes WWE to his children,...</td>\n",
       "      <td>Can we send original jewellery to Qatar from I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105773</th>\n",
       "      <td>105773</td>\n",
       "      <td>174359</td>\n",
       "      <td>32337</td>\n",
       "      <td>What is a stock market? What actually takes pl...</td>\n",
       "      <td>What is sensex all about and how to understand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105774</th>\n",
       "      <td>105774</td>\n",
       "      <td>71504</td>\n",
       "      <td>33701</td>\n",
       "      <td>What is love? How can we find that we are in l...</td>\n",
       "      <td>What is the true meaning of love?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105775</th>\n",
       "      <td>105775</td>\n",
       "      <td>19546</td>\n",
       "      <td>14298</td>\n",
       "      <td>Can we time travel?</td>\n",
       "      <td>Is time travel possible? If yes how</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105776</th>\n",
       "      <td>105776</td>\n",
       "      <td>5122</td>\n",
       "      <td>526</td>\n",
       "      <td>What is/are your New Year resolutions for 2017?</td>\n",
       "      <td>What is your creative New Year's resolution fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105777</th>\n",
       "      <td>105777</td>\n",
       "      <td>18187</td>\n",
       "      <td>38545</td>\n",
       "      <td>How can a person control anger?</td>\n",
       "      <td>How do I control my anger and have patience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105778</th>\n",
       "      <td>105778</td>\n",
       "      <td>174360</td>\n",
       "      <td>174361</td>\n",
       "      <td>DC Comics: Are Deadshot and Deathstroke differ...</td>\n",
       "      <td>Who is Deimos in DC comics?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105779</th>\n",
       "      <td>105779</td>\n",
       "      <td>117426</td>\n",
       "      <td>174362</td>\n",
       "      <td>Which is the most motivational video on YouTube?</td>\n",
       "      <td>What are some of the best motivational videos ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105780</th>\n",
       "      <td>105780</td>\n",
       "      <td>174363</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105781</th>\n",
       "      <td>105781</td>\n",
       "      <td>174365</td>\n",
       "      <td>107209</td>\n",
       "      <td>What are the best short films on YouTube?</td>\n",
       "      <td>What are some of the best short films availabl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105782</th>\n",
       "      <td>105782</td>\n",
       "      <td>10402</td>\n",
       "      <td>1896</td>\n",
       "      <td>Division by Zero: If 1/1 equals 1, 2/2 equals ...</td>\n",
       "      <td>Why does zero factorial (0!) equal one (1)?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105783</th>\n",
       "      <td>105783</td>\n",
       "      <td>174366</td>\n",
       "      <td>48913</td>\n",
       "      <td>How do I find out whos asking anonymous quest...</td>\n",
       "      <td>Is it possible to see when a Quora question wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105784</th>\n",
       "      <td>105784</td>\n",
       "      <td>3750</td>\n",
       "      <td>3062</td>\n",
       "      <td>Is World War III coming?</td>\n",
       "      <td>How close is a World War III?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105785 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0            0       1       2   \n",
       "1            1       3       4   \n",
       "2            2       5       6   \n",
       "3            3       7       8   \n",
       "4            4       9      10   \n",
       "5            5      11      12   \n",
       "6            6      13      14   \n",
       "7            7      15      16   \n",
       "8            8      17      18   \n",
       "9            9      19      20   \n",
       "10          10      21      22   \n",
       "11          11      23      24   \n",
       "12          12      25      26   \n",
       "13          13      27      28   \n",
       "14          14      29      30   \n",
       "15          15      31      32   \n",
       "16          16      33      34   \n",
       "17          17      35      36   \n",
       "18          18      37      38   \n",
       "19          19      39      40   \n",
       "20          20      41      42   \n",
       "21          21      43      44   \n",
       "22          22      45      46   \n",
       "23          23      47      48   \n",
       "24          24      49      50   \n",
       "25          25      51      52   \n",
       "26          26      53      54   \n",
       "27          27      55      56   \n",
       "28          28      57      58   \n",
       "29          29      59      60   \n",
       "...        ...     ...     ...   \n",
       "105755  105755  174332  174333   \n",
       "105756  105756  174334   68791   \n",
       "105757  105757  174335   84291   \n",
       "105758  105758  174336  174337   \n",
       "105759  105759  174338  174339   \n",
       "105760  105760  174340  158724   \n",
       "105761  105761  174341  174342   \n",
       "105762  105762  174343  174344   \n",
       "105763  105763  174345  174346   \n",
       "105764  105764  106802  174347   \n",
       "105765  105765   47020   42108   \n",
       "105766  105766   24372  174348   \n",
       "105767  105767  174349   79077   \n",
       "105768  105768  174350  174351   \n",
       "105769  105769  174352  174353   \n",
       "105770  105770  174354  174355   \n",
       "105771  105771  173453  174356   \n",
       "105772  105772  174357  174358   \n",
       "105773  105773  174359   32337   \n",
       "105774  105774   71504   33701   \n",
       "105775  105775   19546   14298   \n",
       "105776  105776    5122     526   \n",
       "105777  105777   18187   38545   \n",
       "105778  105778  174360  174361   \n",
       "105779  105779  117426  174362   \n",
       "105780  105780  174363  174364   \n",
       "105781  105781  174365  107209   \n",
       "105782  105782   10402    1896   \n",
       "105783  105783  174366   48913   \n",
       "105784  105784    3750    3062   \n",
       "\n",
       "                                                question1  \\\n",
       "0       What is the step by step guide to invest in sh...   \n",
       "1       What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2       How can I increase the speed of my internet co...   \n",
       "3       Why am I mentally very lonely? How can I solve...   \n",
       "4       Which one dissolve in water quikly sugar, salt...   \n",
       "5       Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                     Should I buy tiago?   \n",
       "7                          How can I be a good geologist?   \n",
       "8                         When do you use  instead of ?   \n",
       "9       Motorola (company): Can I hack my Charter Moto...   \n",
       "10      Method to find separation of slits using fresn...   \n",
       "11            How do I read and find my YouTube comments?   \n",
       "12                   What can make Physics easy to learn?   \n",
       "13            What was your first sexual experience like?   \n",
       "14      What are the laws to change your status from a...   \n",
       "15      What would a Trump presidency mean for current...   \n",
       "16                           What does manipulation mean?   \n",
       "17      Why do girls want to be friends with the guy t...   \n",
       "18      Why are so many Quora users posting questions ...   \n",
       "19      Which is the best digital marketing institutio...   \n",
       "20                             Why do rockets look white?   \n",
       "21                  What's causing someone to be jealous?   \n",
       "22        What are the questions should not ask on Quora?   \n",
       "23                               How much is 30 kV in HP?   \n",
       "24      What does it mean that every time I look at th...   \n",
       "25      What are some tips on making it through the jo...   \n",
       "26                               What is web application?   \n",
       "27      Does society place too much importance on sports?   \n",
       "28                 What is best way to make money online?   \n",
       "29                 How should I prepare for CA final law?   \n",
       "...                                                   ...   \n",
       "105755  Should aspiring billionaires be as multifacete...   \n",
       "105756       What is the purpose of a board of directors?   \n",
       "105757  Is now a good time to invest in the Melbourne ...   \n",
       "105758                            Why do I like violence?   \n",
       "105759       What is your opinion about pair programming?   \n",
       "105760                               Why are oceans blue?   \n",
       "105761  Which torrent allows downloading paid ebooks f...   \n",
       "105762                      How can I get money for free?   \n",
       "105763  Which country is best to study in: Canada or A...   \n",
       "105764          Why would parents abandon their children?   \n",
       "105765  Why do people write questions on Quora that co...   \n",
       "105766  How do I get a Starbucks gold card and what do...   \n",
       "105767            Is majoring in economics a good choice?   \n",
       "105768  What are the differences between the Shaytan o...   \n",
       "105769      What is it like to be a professional flutist?   \n",
       "105770  What does the U.S. Navy's motto \"semper fortis...   \n",
       "105771  What was Barack Obama's GPA in high school and...   \n",
       "105772  When Vince McMahon passes WWE to his children,...   \n",
       "105773  What is a stock market? What actually takes pl...   \n",
       "105774  What is love? How can we find that we are in l...   \n",
       "105775                                Can we time travel?   \n",
       "105776    What is/are your New Year resolutions for 2017?   \n",
       "105777                    How can a person control anger?   \n",
       "105778  DC Comics: Are Deadshot and Deathstroke differ...   \n",
       "105779   Which is the most motivational video on YouTube?   \n",
       "105780                     How can I develop android app?   \n",
       "105781          What are the best short films on YouTube?   \n",
       "105782  Division by Zero: If 1/1 equals 1, 2/2 equals ...   \n",
       "105783  How do I find out whos asking anonymous quest...   \n",
       "105784                           Is World War III coming?   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       What is the step by step guide to invest in sh...             0  \n",
       "1       What would happen if the Indian government sto...             0  \n",
       "2       How can Internet speed be increased by hacking...             0  \n",
       "3       Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4                 Which fish would survive in salt water?             0  \n",
       "5       I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6       What keeps childern active and far from phone ...             0  \n",
       "7               What should I do to be a great geologist?             1  \n",
       "8                   When do you use \"&\" instead of \"and\"?             0  \n",
       "9       How do I hack Motorola DCX3400 for free internet?             0  \n",
       "10      What are some of the things technicians can te...             0  \n",
       "11                 How can I see all my Youtube comments?             1  \n",
       "12                How can you make physics easy to learn?             1  \n",
       "13                 What was your first sexual experience?             1  \n",
       "14      What are the laws to change your status from a...             0  \n",
       "15      How will a Trump presidency affect the student...             1  \n",
       "16                          What does manipulation means?             1  \n",
       "17               How do guys feel after rejecting a girl?             0  \n",
       "18      Why do people ask Quora questions which can be...             1  \n",
       "19      Which is the best digital marketing institute ...             0  \n",
       "20            Why are rockets and boosters painted white?             1  \n",
       "21       What can I do to avoid being jealous of someone?             0  \n",
       "22                  Which question should I ask on Quora?             0  \n",
       "23      Where can I find a conversion chart for CC to ...             0  \n",
       "24       How many times a day do a clocks hands overlap?             0  \n",
       "25      What are some tips on making it through the jo...             0  \n",
       "26                 What is the web application framework?             0  \n",
       "27               How do sports contribute to the society?             0  \n",
       "28              What is best way to ask for money online?             0  \n",
       "29      How one should know that he/she completely pre...             1  \n",
       "...                                                   ...           ...  \n",
       "105755  Can you tell that someone has been looking at ...             0  \n",
       "105756  What are the responsibilities of a board of di...             1  \n",
       "105757  Is now a good time to invest in the California...             0  \n",
       "105758                Why is there violence in the world?             0  \n",
       "105759             What do you think of pair programming?             1  \n",
       "105760                               Why is the sea blue?             1  \n",
       "105761       Does Google pay when your app is downloaded?             0  \n",
       "105762                           How do I get free money?             1  \n",
       "105763  Which country holds the best career opportunit...             0  \n",
       "105764             Why do children disobey their parents?             0  \n",
       "105765  Why do people use Quora when they could easily...             1  \n",
       "105766  Does a US Starbucks Card work in other countries?             0  \n",
       "105767          Is graduation in economics a good choice?             1  \n",
       "105768  How do the Muslim beliefs about Shaitan parall...             0  \n",
       "105769             Where can I sell windows product keys?             0  \n",
       "105770                       What is Navy Boot Camp like?             0  \n",
       "105771        What GPA did Bill Gates get in high school?             0  \n",
       "105772  Can we send original jewellery to Qatar from I...             0  \n",
       "105773  What is sensex all about and how to understand...             1  \n",
       "105774                  What is the true meaning of love?             1  \n",
       "105775                Is time travel possible? If yes how             1  \n",
       "105776  What is your creative New Year's resolution fo...             1  \n",
       "105777       How do I control my anger and have patience?             1  \n",
       "105778                        Who is Deimos in DC comics?             0  \n",
       "105779  What are some of the best motivational videos ...             1  \n",
       "105780                                                                0  \n",
       "105781  What are some of the best short films availabl...             0  \n",
       "105782        Why does zero factorial (0!) equal one (1)?             0  \n",
       "105783  Is it possible to see when a Quora question wa...             0  \n",
       "105784                      How close is a World War III?             1  \n",
       "\n",
       "[105785 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_quora_train = quora_train.drop(\"is_duplicate\", axis=1)\n",
    "y_quora_train = quora_train[\"is_duplicate\"]\n",
    "x_quora_test = quora_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save pickle with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train_model_file = 'quora_train_model.pkl'\n",
    "quora_test_model_file = 'quora_test_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(quora_train, open(quora_train_model_file, 'wb'))\n",
    "pickle.dump(quora_test, open(quora_test_model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train_model = pickle.load(open(quora_train_model_file, 'rb'))\n",
    "quora_test_model = pickle.load(open(quora_test_model_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.030325</td>\n",
       "      <td>-0.049192</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.035923</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>-0.032556</td>\n",
       "      <td>-0.005223</td>\n",
       "      <td>0.007918</td>\n",
       "      <td>0.037580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>-36</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>-0.027570</td>\n",
       "      <td>0.017795</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>-0.014061</td>\n",
       "      <td>-0.008328</td>\n",
       "      <td>-0.039656</td>\n",
       "      <td>-0.045824</td>\n",
       "      <td>0.024256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.040165</td>\n",
       "      <td>-0.025984</td>\n",
       "      <td>-0.028276</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>0.032029</td>\n",
       "      <td>-0.039819</td>\n",
       "      <td>-0.011250</td>\n",
       "      <td>0.034087</td>\n",
       "      <td>-0.005311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>-16</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012875</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>0.014786</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.037780</td>\n",
       "      <td>-0.017801</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>-0.004514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>-0.040094</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.005606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>-0.023660</td>\n",
       "      <td>0.009514</td>\n",
       "      <td>0.032591</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>-0.007625</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-0.060255</td>\n",
       "      <td>-0.004745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>-34</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009289</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>-0.033773</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>-0.006809</td>\n",
       "      <td>-0.005909</td>\n",
       "      <td>-0.016265</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>-0.025786</td>\n",
       "      <td>0.019409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010061</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>0.058093</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>-0.003913</td>\n",
       "      <td>-0.053085</td>\n",
       "      <td>-0.026273</td>\n",
       "      <td>0.014160</td>\n",
       "      <td>0.006114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.027967</td>\n",
       "      <td>0.022788</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>-0.008064</td>\n",
       "      <td>-0.047973</td>\n",
       "      <td>-0.008863</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>-0.009627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>-0.027905</td>\n",
       "      <td>-0.018013</td>\n",
       "      <td>0.018447</td>\n",
       "      <td>-0.029490</td>\n",
       "      <td>-0.016252</td>\n",
       "      <td>-0.023360</td>\n",
       "      <td>-0.019657</td>\n",
       "      <td>-0.033325</td>\n",
       "      <td>0.023413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>-15</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001650</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>-0.022767</td>\n",
       "      <td>-0.008851</td>\n",
       "      <td>-0.005842</td>\n",
       "      <td>0.019220</td>\n",
       "      <td>-0.009135</td>\n",
       "      <td>0.018288</td>\n",
       "      <td>-0.020233</td>\n",
       "      <td>-0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015367</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>-0.008826</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>-0.029646</td>\n",
       "      <td>-0.004322</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>-0.026758</td>\n",
       "      <td>-0.008969</td>\n",
       "      <td>0.029338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011117</td>\n",
       "      <td>-0.006091</td>\n",
       "      <td>-0.037324</td>\n",
       "      <td>-0.017701</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>-0.022123</td>\n",
       "      <td>0.030317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001715</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>-0.047358</td>\n",
       "      <td>-0.041954</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.071346</td>\n",
       "      <td>0.031369</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.032631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>0.014282</td>\n",
       "      <td>-0.052805</td>\n",
       "      <td>-0.014114</td>\n",
       "      <td>0.023064</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>-0.026530</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.030364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025924</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.039654</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>-0.039029</td>\n",
       "      <td>0.012655</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>0.007119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>-0.020706</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>-0.010335</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>-0.040959</td>\n",
       "      <td>0.016808</td>\n",
       "      <td>-0.024346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015255</td>\n",
       "      <td>0.025267</td>\n",
       "      <td>-0.035017</td>\n",
       "      <td>-0.010091</td>\n",
       "      <td>-0.010212</td>\n",
       "      <td>-0.016371</td>\n",
       "      <td>-0.009538</td>\n",
       "      <td>-0.042712</td>\n",
       "      <td>-0.021967</td>\n",
       "      <td>-0.049092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>0.021481</td>\n",
       "      <td>-0.025368</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>-0.010714</td>\n",
       "      <td>-0.035070</td>\n",
       "      <td>-0.002737</td>\n",
       "      <td>-0.019209</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.019007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>-0.011217</td>\n",
       "      <td>-0.028839</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>0.030085</td>\n",
       "      <td>-0.021143</td>\n",
       "      <td>-0.026680</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>-0.006366</td>\n",
       "      <td>0.005053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>-12</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>-0.010879</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>-0.012498</td>\n",
       "      <td>0.038511</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0.027773</td>\n",
       "      <td>-0.016397</td>\n",
       "      <td>-0.005503</td>\n",
       "      <td>-0.019987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029361</td>\n",
       "      <td>0.036366</td>\n",
       "      <td>-0.005358</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.030230</td>\n",
       "      <td>-0.010403</td>\n",
       "      <td>-0.031823</td>\n",
       "      <td>-0.008357</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>-0.016588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>-0.069607</td>\n",
       "      <td>0.071575</td>\n",
       "      <td>-0.035602</td>\n",
       "      <td>-0.027701</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.002342</td>\n",
       "      <td>-0.012004</td>\n",
       "      <td>0.021819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>35</td>\n",
       "      <td>-22</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014158</td>\n",
       "      <td>-0.004881</td>\n",
       "      <td>-0.019231</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>-0.007847</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.022773</td>\n",
       "      <td>-0.023311</td>\n",
       "      <td>-0.020713</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>-0.026265</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>-0.034537</td>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>53</td>\n",
       "      <td>-10</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>-0.022836</td>\n",
       "      <td>-0.013566</td>\n",
       "      <td>-0.017568</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>-0.024321</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>-0.028073</td>\n",
       "      <td>0.018159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062652</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>-0.030531</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>-0.007220</td>\n",
       "      <td>0.015807</td>\n",
       "      <td>-0.022308</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>-0.005275</td>\n",
       "      <td>0.080592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073239</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>-0.011728</td>\n",
       "      <td>-0.013976</td>\n",
       "      <td>0.043751</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>-0.038621</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.055771</td>\n",
       "      <td>-0.002967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019027</td>\n",
       "      <td>-0.011016</td>\n",
       "      <td>-0.054900</td>\n",
       "      <td>-0.016064</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>-0.003319</td>\n",
       "      <td>-0.020878</td>\n",
       "      <td>0.040597</td>\n",
       "      <td>-0.054297</td>\n",
       "      <td>0.002482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>-27</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>-0.030521</td>\n",
       "      <td>-0.025856</td>\n",
       "      <td>0.011169</td>\n",
       "      <td>-0.011129</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>-0.009966</td>\n",
       "      <td>0.023335</td>\n",
       "      <td>-0.053070</td>\n",
       "      <td>0.017634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404260</th>\n",
       "      <td>404260</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>-2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036258</td>\n",
       "      <td>0.028151</td>\n",
       "      <td>-0.061801</td>\n",
       "      <td>-0.039161</td>\n",
       "      <td>-0.040540</td>\n",
       "      <td>-0.022131</td>\n",
       "      <td>-0.030469</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>-0.050010</td>\n",
       "      <td>-0.011283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404261</th>\n",
       "      <td>404261</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046967</td>\n",
       "      <td>0.039472</td>\n",
       "      <td>-0.052129</td>\n",
       "      <td>0.018586</td>\n",
       "      <td>0.028658</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>-0.015020</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.055655</td>\n",
       "      <td>-0.006925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404262</th>\n",
       "      <td>404262</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014477</td>\n",
       "      <td>0.028248</td>\n",
       "      <td>-0.030208</td>\n",
       "      <td>-0.008479</td>\n",
       "      <td>0.025596</td>\n",
       "      <td>-0.011012</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>-0.014609</td>\n",
       "      <td>-0.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404263</th>\n",
       "      <td>404263</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028542</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.018260</td>\n",
       "      <td>-0.009013</td>\n",
       "      <td>0.041275</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>-0.043033</td>\n",
       "      <td>0.014150</td>\n",
       "      <td>0.043295</td>\n",
       "      <td>0.031648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404264</th>\n",
       "      <td>404264</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001616</td>\n",
       "      <td>0.014709</td>\n",
       "      <td>-0.006176</td>\n",
       "      <td>-0.001149</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>-0.017364</td>\n",
       "      <td>-0.008077</td>\n",
       "      <td>-0.068696</td>\n",
       "      <td>-0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404265</th>\n",
       "      <td>404265</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>-0.006588</td>\n",
       "      <td>0.041896</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.055975</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>-0.032512</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.037388</td>\n",
       "      <td>0.038587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404266</th>\n",
       "      <td>404266</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>120</td>\n",
       "      <td>-85</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010212</td>\n",
       "      <td>0.018669</td>\n",
       "      <td>-0.017065</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>-0.036763</td>\n",
       "      <td>-0.013983</td>\n",
       "      <td>-0.016696</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404267</th>\n",
       "      <td>404267</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>-18</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>0.078260</td>\n",
       "      <td>-0.045504</td>\n",
       "      <td>-0.007726</td>\n",
       "      <td>0.032950</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>-0.042852</td>\n",
       "      <td>-0.006852</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.014164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404268</th>\n",
       "      <td>404268</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030269</td>\n",
       "      <td>0.046676</td>\n",
       "      <td>-0.014227</td>\n",
       "      <td>-0.021084</td>\n",
       "      <td>0.022709</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.050913</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>0.018146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404269</th>\n",
       "      <td>404269</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016475</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>-0.009689</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.037268</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>-0.019307</td>\n",
       "      <td>-0.004633</td>\n",
       "      <td>-0.021614</td>\n",
       "      <td>-0.023879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404270</th>\n",
       "      <td>404270</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>-0.032157</td>\n",
       "      <td>0.013731</td>\n",
       "      <td>0.008410</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>-0.033148</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>-0.003496</td>\n",
       "      <td>-0.007620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404271</th>\n",
       "      <td>404271</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005055</td>\n",
       "      <td>-0.010115</td>\n",
       "      <td>-0.050893</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>0.035056</td>\n",
       "      <td>0.025329</td>\n",
       "      <td>-0.014011</td>\n",
       "      <td>-0.007668</td>\n",
       "      <td>-0.022039</td>\n",
       "      <td>-0.033961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404272</th>\n",
       "      <td>404272</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016506</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>-0.059710</td>\n",
       "      <td>-0.010088</td>\n",
       "      <td>0.033432</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>-0.057210</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>-0.017429</td>\n",
       "      <td>-0.022020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404273</th>\n",
       "      <td>404273</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>63</td>\n",
       "      <td>-34</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>-0.034040</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>-0.003109</td>\n",
       "      <td>-0.019376</td>\n",
       "      <td>-0.045262</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>0.034657</td>\n",
       "      <td>0.012897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404274</th>\n",
       "      <td>404274</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404275</th>\n",
       "      <td>404275</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>-4</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034237</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>-0.033764</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>-0.007956</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>-0.019392</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.031013</td>\n",
       "      <td>0.034991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404276</th>\n",
       "      <td>404276</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012015</td>\n",
       "      <td>-0.024424</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>-0.020761</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.029839</td>\n",
       "      <td>-0.034735</td>\n",
       "      <td>0.063526</td>\n",
       "      <td>-0.035226</td>\n",
       "      <td>-0.016951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404277</th>\n",
       "      <td>404277</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>-10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009303</td>\n",
       "      <td>0.021561</td>\n",
       "      <td>-0.003227</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.000631</td>\n",
       "      <td>0.039937</td>\n",
       "      <td>-0.036282</td>\n",
       "      <td>0.015419</td>\n",
       "      <td>-0.020105</td>\n",
       "      <td>0.009312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404278</th>\n",
       "      <td>404278</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>-9</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007725</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.013542</td>\n",
       "      <td>0.045067</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>-0.000593</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>-0.006893</td>\n",
       "      <td>0.006996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404279</th>\n",
       "      <td>404279</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "      <td>-8</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>-0.010451</td>\n",
       "      <td>-0.019210</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>-0.013658</td>\n",
       "      <td>-0.020676</td>\n",
       "      <td>0.023245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404280</th>\n",
       "      <td>404280</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>-7</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>-0.005136</td>\n",
       "      <td>-0.003422</td>\n",
       "      <td>-0.018666</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>-0.038352</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>0.034933</td>\n",
       "      <td>0.033354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404281</th>\n",
       "      <td>404281</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>-0.035284</td>\n",
       "      <td>-0.018816</td>\n",
       "      <td>-0.018709</td>\n",
       "      <td>-0.019806</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>-0.042604</td>\n",
       "      <td>-0.012778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404282</th>\n",
       "      <td>404282</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>-7</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>-0.030224</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.051462</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>-0.024898</td>\n",
       "      <td>-0.009689</td>\n",
       "      <td>0.057215</td>\n",
       "      <td>-0.010018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404283</th>\n",
       "      <td>404283</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>39</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>-0.038892</td>\n",
       "      <td>-0.028352</td>\n",
       "      <td>0.012241</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>0.036264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404284</th>\n",
       "      <td>404284</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>-5</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>-0.005394</td>\n",
       "      <td>-0.038470</td>\n",
       "      <td>0.036069</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.033403</td>\n",
       "      <td>-0.043255</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>-0.032982</td>\n",
       "      <td>-0.024281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>404285</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>0.040150</td>\n",
       "      <td>-0.045695</td>\n",
       "      <td>-0.014050</td>\n",
       "      <td>-0.003148</td>\n",
       "      <td>-0.052308</td>\n",
       "      <td>-0.001239</td>\n",
       "      <td>0.057605</td>\n",
       "      <td>-0.091402</td>\n",
       "      <td>-0.013218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>404286</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>-0.017259</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>-0.019421</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.001631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404287</th>\n",
       "      <td>404287</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020040</td>\n",
       "      <td>0.053471</td>\n",
       "      <td>-0.022978</td>\n",
       "      <td>-0.031842</td>\n",
       "      <td>0.020518</td>\n",
       "      <td>0.045978</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>-0.023306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404288</th>\n",
       "      <td>404288</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>-16</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404289</th>\n",
       "      <td>404289</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404288 rows  417 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  is_duplicate  len_q1  len_q2  diff_len  len_char_q1  \\\n",
       "0            0             0      36      30         6           15   \n",
       "1            1             0      31      67       -36           12   \n",
       "2            2             0      44      36         8           14   \n",
       "3            3             0      24      40       -16           11   \n",
       "4            4             0      60      29        31           22   \n",
       "5            5             1      54      55        -1           17   \n",
       "6            6             0       9      43       -34            8   \n",
       "7            7             1      14      15        -1            8   \n",
       "8            8             0      15      16        -1           10   \n",
       "9            9             0      47      35        12           18   \n",
       "10          10             0      50      65       -15           17   \n",
       "11          11             1      26      20         6           15   \n",
       "12          12             1      23      23         0           13   \n",
       "13          13             1      28      23         5           14   \n",
       "14          14             0      72      71         1           18   \n",
       "15          15             1      75      63        12           20   \n",
       "16          16             1      17      18        -1           10   \n",
       "17          17             0      29      24         5           16   \n",
       "18          18             1      58      49         9           17   \n",
       "19          19             0      43      37         6           15   \n",
       "20          20             1      18      30       -12           11   \n",
       "21          21             0      29      21         8           15   \n",
       "22          22             0      19      18         1           11   \n",
       "23          23             0      13      35       -22            9   \n",
       "24          24             0      39      36         3           16   \n",
       "25          25             0      43      53       -10           18   \n",
       "26          26             0      15      25       -10           11   \n",
       "27          27             0      36      25        11           15   \n",
       "28          28             0      26      25         1           13   \n",
       "29          29             1      20      47       -27           10   \n",
       "...        ...           ...     ...     ...       ...          ...   \n",
       "404260  404260             0      16      18        -2           11   \n",
       "404261  404261             1      38      37         1           15   \n",
       "404262  404262             0      27      20         7           12   \n",
       "404263  404263             0      46      42         4           16   \n",
       "404264  404264             0      58      26        32           20   \n",
       "404265  404265             1      16      15         1           12   \n",
       "404266  404266             0      35     120       -85           14   \n",
       "404267  404267             1      24      42       -18           13   \n",
       "404268  404268             0      35      27         8           18   \n",
       "404269  404269             0      40      41        -1           14   \n",
       "404270  404270             0      14      15        -1           10   \n",
       "404271  404271             0      42      40         2           16   \n",
       "404272  404272             1      55      55         0           17   \n",
       "404273  404273             1      29      63       -34           16   \n",
       "404274  404274             1      42      33         9           15   \n",
       "404275  404275             0      41      45        -4           16   \n",
       "404276  404276             0      16      14         2           12   \n",
       "404277  404277             0      12      22       -10           10   \n",
       "404278  404278             0      22      31        -9            9   \n",
       "404279  404279             0      39      47        -8           17   \n",
       "404280  404280             1      28      35        -7           13   \n",
       "404281  404281             1      42      42         0           18   \n",
       "404282  404282             1      31      38        -7           16   \n",
       "404283  404283             0      55      39        16           22   \n",
       "404284  404284             1      25      30        -5           14   \n",
       "404285  404285             0      56      54         2           19   \n",
       "404286  404286             1      18      15         3           10   \n",
       "404287  404287             0       8      10        -2            5   \n",
       "404288  404288             0      61      77       -16           18   \n",
       "404289  404289             0      15      15         0           10   \n",
       "\n",
       "        len_char_q2  len_word_q1  len_word_q2  common_words    ...     \\\n",
       "0                15            6            5             5    ...      \n",
       "1                20            4            9             2    ...      \n",
       "2                13            6            5             2    ...      \n",
       "3                14            4            5             0    ...      \n",
       "4                14           10            5             2    ...      \n",
       "5                15            9            9             5    ...      \n",
       "6                18            2            7             0    ...      \n",
       "7                 9            2            2             1    ...      \n",
       "8                 8            4            3             2    ...      \n",
       "9                18            6            5             3    ...      \n",
       "10               18            7            7             0    ...      \n",
       "11               10            4            3             2    ...      \n",
       "12               13            4            4             4    ...      \n",
       "13               13            4            3             3    ...      \n",
       "14               19           11           11            10    ...      \n",
       "15               16           10            9             3    ...      \n",
       "16               11            2            2             1    ...      \n",
       "17               13            5            4             0    ...      \n",
       "18               17            8            7             4    ...      \n",
       "19               15            5            5             3    ...      \n",
       "20               15            3            4             2    ...      \n",
       "21               12            4            3             2    ...      \n",
       "22               11            3            3             2    ...      \n",
       "23               15            4            5             0    ...      \n",
       "24               18            7            6             0    ...      \n",
       "25               20            6            7             5    ...      \n",
       "26               15            2            3             2    ...      \n",
       "27               12            5            3             2    ...      \n",
       "28               13            5            5             4    ...      \n",
       "29               18            4            8             3    ...      \n",
       "...             ...          ...          ...           ...    ...      \n",
       "404260           13            3            4             2    ...      \n",
       "404261           15            5            5             4    ...      \n",
       "404262           11            3            3             2    ...      \n",
       "404263           17            6            6             3    ...      \n",
       "404264           14           10            5             3    ...      \n",
       "404265            8            3            3             2    ...      \n",
       "404266           23            6           19             3    ...      \n",
       "404267           14            4            6             4    ...      \n",
       "404268           17            7            5             2    ...      \n",
       "404269           15            4            5             3    ...      \n",
       "404270            8            2            2             1    ...      \n",
       "404271           17            5            7             0    ...      \n",
       "404272           20            9            8             4    ...      \n",
       "404273           18            5           10             4    ...      \n",
       "404274           15            6            5             2    ...      \n",
       "404275           15            5            6             1    ...      \n",
       "404276            9            1            1             0    ...      \n",
       "404277           11            2            3             0    ...      \n",
       "404278           13            2            5             1    ...      \n",
       "404279           20            6            7             2    ...      \n",
       "404280           15            5            6             5    ...      \n",
       "404281           15            6            6             4    ...      \n",
       "404282           15            4            4             2    ...      \n",
       "404283           18            8            7             3    ...      \n",
       "404284           15            3            4             3    ...      \n",
       "404285           18            7            7             6    ...      \n",
       "404286           10            3            3             2    ...      \n",
       "404287            9            2            2             1    ...      \n",
       "404288           21            9           13             0    ...      \n",
       "404289           10            3            3             3    ...      \n",
       "\n",
       "             390       391       392       393       394       395       396  \\\n",
       "0       0.032900  0.030325 -0.049192  0.003424  0.035923  0.008304 -0.032556   \n",
       "1       0.001686  0.006481 -0.027570  0.017795  0.003547 -0.014061 -0.008328   \n",
       "2       0.000535  0.040165 -0.025984 -0.028276 -0.004367  0.032029 -0.039819   \n",
       "3      -0.012875  0.008011 -0.022419  0.014786 -0.002817  0.037780 -0.017801   \n",
       "4       0.030153 -0.005706 -0.040094  0.040029  0.016195  0.012857 -0.014082   \n",
       "5      -0.001553  0.016845 -0.023660  0.009514  0.032591 -0.012500 -0.007625   \n",
       "6      -0.009289  0.008985 -0.033773  0.003934 -0.006809 -0.005909 -0.016265   \n",
       "7      -0.010061  0.020083  0.020137  0.058093  0.027359 -0.003913 -0.053085   \n",
       "8       0.027580  0.027967  0.022788 -0.028317  0.051493 -0.008064 -0.047973   \n",
       "9       0.027451 -0.027905 -0.018013  0.018447 -0.029490 -0.016252 -0.023360   \n",
       "10     -0.001650  0.034401 -0.022767 -0.008851 -0.005842  0.019220 -0.009135   \n",
       "11     -0.015367  0.007261 -0.008826  0.021349 -0.029646 -0.004322  0.009627   \n",
       "12     -0.011117 -0.006091 -0.037324 -0.017701  0.009830  0.027737  0.016444   \n",
       "13     -0.001715 -0.005170 -0.047358 -0.041954  0.009401 -0.001696 -0.071346   \n",
       "14      0.007736  0.014282 -0.052805 -0.014114  0.023064  0.003095 -0.026530   \n",
       "15      0.025924 -0.000776 -0.039654 -0.000531  0.009896 -0.008635 -0.039029   \n",
       "16      0.039217  0.026385 -0.020706  0.012067 -0.010335  0.004962  0.014328   \n",
       "17     -0.015255  0.025267 -0.035017 -0.010091 -0.010212 -0.016371 -0.009538   \n",
       "18      0.022351  0.021481 -0.025368  0.013348 -0.010714 -0.035070 -0.002737   \n",
       "19      0.024540 -0.011217 -0.028839 -0.011763  0.030085 -0.021143 -0.026680   \n",
       "20      0.002892 -0.010879  0.004611 -0.012498  0.038511 -0.000523  0.027773   \n",
       "21     -0.029361  0.036366 -0.005358  0.007829  0.030230 -0.010403 -0.031823   \n",
       "22      0.040252  0.017749 -0.069607  0.071575 -0.035602 -0.027701 -0.024815   \n",
       "23     -0.014158 -0.004881 -0.019231  0.000106  0.016315 -0.004817 -0.007847   \n",
       "24      0.009021  0.022773 -0.023311 -0.020713  0.031486  0.004416 -0.026265   \n",
       "25      0.026328 -0.022836 -0.013566 -0.017568  0.026021  0.009996 -0.024321   \n",
       "26      0.062652  0.012252 -0.030531  0.008526 -0.007220  0.015807 -0.022308   \n",
       "27      0.073239  0.004871 -0.011728 -0.013976  0.043751  0.012245 -0.038621   \n",
       "28     -0.019027 -0.011016 -0.054900 -0.016064  0.025419 -0.003319 -0.020878   \n",
       "29      0.005990 -0.030521 -0.025856  0.011169 -0.011129  0.017751 -0.009966   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "404260 -0.036258  0.028151 -0.061801 -0.039161 -0.040540 -0.022131 -0.030469   \n",
       "404261  0.046967  0.039472 -0.052129  0.018586  0.028658  0.029272 -0.015020   \n",
       "404262 -0.014477  0.028248 -0.030208 -0.008479  0.025596 -0.011012  0.000039   \n",
       "404263  0.028542  0.009628  0.018260 -0.009013  0.041275  0.009764 -0.043033   \n",
       "404264 -0.001616  0.014709 -0.006176 -0.001149  0.017293  0.004005 -0.017364   \n",
       "404265  0.038036 -0.006588  0.041896  0.027090  0.055975  0.021234 -0.032512   \n",
       "404266 -0.010212  0.018669 -0.017065  0.023331  0.038721  0.008422 -0.036763   \n",
       "404267  0.014850  0.078260 -0.045504 -0.007726  0.032950  0.003860 -0.042852   \n",
       "404268  0.030269  0.046676 -0.014227 -0.021084  0.022709  0.047607 -0.050913   \n",
       "404269 -0.016475  0.007042 -0.009689  0.008308  0.037268 -0.018555 -0.019307   \n",
       "404270  0.008384  0.011681 -0.032157  0.013731  0.008410  0.015873 -0.033148   \n",
       "404271 -0.005055 -0.010115 -0.050893 -0.003564  0.035056  0.025329 -0.014011   \n",
       "404272  0.016506  0.028229 -0.059710 -0.010088  0.033432  0.004451 -0.057210   \n",
       "404273  0.010456  0.009869 -0.034040  0.007183 -0.003109 -0.019376 -0.045262   \n",
       "404274  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "404275  0.034237  0.003757 -0.033764  0.025408 -0.007956  0.007484 -0.019392   \n",
       "404276 -0.012015 -0.024424 -0.001922 -0.020761  0.008130  0.029839 -0.034735   \n",
       "404277 -0.009303  0.021561 -0.003227 -0.000754 -0.000631  0.039937 -0.036282   \n",
       "404278 -0.007725  0.007206 -0.017172 -0.013542  0.045067 -0.017675 -0.000593   \n",
       "404279  0.002103 -0.010451 -0.019210 -0.026655  0.024507  0.025140  0.003731   \n",
       "404280  0.006768  0.039011 -0.005136 -0.003422 -0.018666  0.041606 -0.038352   \n",
       "404281  0.009827  0.005058 -0.035284 -0.018816 -0.018709 -0.019806 -0.016057   \n",
       "404282 -0.016037  0.020826 -0.030224  0.007324  0.051462  0.006026 -0.024898   \n",
       "404283  0.016090  0.037500 -0.038892 -0.028352  0.012241  0.002386 -0.012207   \n",
       "404284 -0.002502 -0.005394 -0.038470  0.036069  0.019027  0.033403 -0.043255   \n",
       "404285 -0.001397  0.040150 -0.045695 -0.014050 -0.003148 -0.052308 -0.001239   \n",
       "404286  0.022352  0.011035 -0.017259  0.002073  0.011621  0.012076 -0.019421   \n",
       "404287 -0.020040  0.053471 -0.022978 -0.031842  0.020518  0.045978 -0.000640   \n",
       "404288       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "404289       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "             397       398       399  \n",
       "0      -0.005223  0.007918  0.037580  \n",
       "1      -0.039656 -0.045824  0.024256  \n",
       "2      -0.011250  0.034087 -0.005311  \n",
       "3      -0.011077  0.025301 -0.004514  \n",
       "4       0.006317  0.023808  0.005606  \n",
       "5       0.000968 -0.060255 -0.004745  \n",
       "6       0.022458 -0.025786  0.019409  \n",
       "7      -0.026273  0.014160  0.006114  \n",
       "8      -0.008863  0.018951 -0.009627  \n",
       "9      -0.019657 -0.033325  0.023413  \n",
       "10      0.018288 -0.020233 -0.000366  \n",
       "11     -0.026758 -0.008969  0.029338  \n",
       "12      0.030056 -0.022123  0.030317  \n",
       "13      0.031369  0.006781  0.032631  \n",
       "14     -0.001549  0.003023  0.030364  \n",
       "15      0.012655  0.026634  0.007119  \n",
       "16     -0.040959  0.016808 -0.024346  \n",
       "17     -0.042712 -0.021967 -0.049092  \n",
       "18     -0.019209  0.000352  0.019007  \n",
       "19      0.027567 -0.006366  0.005053  \n",
       "20     -0.016397 -0.005503 -0.019987  \n",
       "21     -0.008357  0.016184 -0.016588  \n",
       "22     -0.002342 -0.012004  0.021819  \n",
       "23      0.025820  0.014393  0.014286  \n",
       "24      0.004108 -0.034537  0.002533  \n",
       "25      0.003722 -0.028073  0.018159  \n",
       "26      0.001506 -0.005275  0.080592  \n",
       "27      0.007113  0.055771 -0.002967  \n",
       "28      0.040597 -0.054297  0.002482  \n",
       "29      0.023335 -0.053070  0.017634  \n",
       "...          ...       ...       ...  \n",
       "404260  0.022842 -0.050010 -0.011283  \n",
       "404261  0.004377  0.055655 -0.006925  \n",
       "404262  0.021118 -0.014609 -0.001677  \n",
       "404263  0.014150  0.043295  0.031648  \n",
       "404264 -0.008077 -0.068696 -0.000403  \n",
       "404265  0.011141  0.037388  0.038587  \n",
       "404266 -0.013983 -0.016696  0.003637  \n",
       "404267 -0.006852  0.006434  0.014164  \n",
       "404268  0.016041  0.045305  0.018146  \n",
       "404269 -0.004633 -0.021614 -0.023879  \n",
       "404270 -0.003910 -0.003496 -0.007620  \n",
       "404271 -0.007668 -0.022039 -0.033961  \n",
       "404272  0.002147 -0.017429 -0.022020  \n",
       "404273  0.032288  0.034657  0.012897  \n",
       "404274  0.000000  0.000000  0.000000  \n",
       "404275  0.006498  0.031013  0.034991  \n",
       "404276  0.063526 -0.035226 -0.016951  \n",
       "404277  0.015419 -0.020105  0.009312  \n",
       "404278  0.007123 -0.006893  0.006996  \n",
       "404279 -0.013658 -0.020676  0.023245  \n",
       "404280 -0.007043  0.034933  0.033354  \n",
       "404281 -0.001817 -0.042604 -0.012778  \n",
       "404282 -0.009689  0.057215 -0.010018  \n",
       "404283 -0.000761 -0.008675  0.036264  \n",
       "404284 -0.004680 -0.032982 -0.024281  \n",
       "404285  0.057605 -0.091402 -0.013218  \n",
       "404286  0.002269  0.005242  0.001631  \n",
       "404287  0.009313  0.003991 -0.023306  \n",
       "404288       NaN       NaN       NaN  \n",
       "404289       NaN       NaN       NaN  \n",
       "\n",
       "[404288 rows x 417 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train_model_file1 = 'quora_train1.pkl'\n",
    "quora_train_model_file2 = 'quora_train2.pkl'\n",
    "#quora_train_model_file3 = 'quora_train3.pkl'\n",
    "quora_train_model_file4 = 'quora_train4.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train1 = pickle.load(open(quora_train_model_file1, 'rb'))\n",
    "quora_train2 = pickle.load(open(quora_train_model_file2, 'rb'))\n",
    "#quora_train3 = pickle.load(open(quora_train_model_file3, 'rb'))\n",
    "quora_train4 = pickle.load(open(quora_train_model_file4, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [quora_train1, quora_train2, quora_train3, quora_train4]\n",
    "\n",
    "quora_train = pandas.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split x_quora_train and y_quora_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_quora_train = quora_train.drop(\"is_duplicate\", axis=1)\n",
    "y_quora_train = quora_train[\"is_duplicate\"]\n",
    "x_quora_test = quora_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=300, n_jobs=-1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest = RandomForestClassifier(n_estimators=300, max_features='auto', bootstrap=False, \n",
    "                               oob_score=False, n_jobs=-1, random_state=0)\n",
    "randomforest.fit(x_quora_train, y_quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = randomforest.predict_proba(x_quora_test)\n",
    "prediction_submission = [i[1] for i in predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345790"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save pickle with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list_file = 'predictions_list.pkl'\n",
    "pickle.dump(prediction_submission, open(predictions_list_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighting predictions\n",
    "Forum discussions showed that the disctibution is imbalaced, with only 20% of the pairs being duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37689944444444451"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mean=numpy.mean(pred_submission)\n",
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighed_pred_submission = []\n",
    "for pred in pred_submission:\n",
    "    weighed_pred=pred*(0.2/pred_mean)\n",
    "    weighed_pred_submission.append(weighed_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(weighed_pred_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create csv file with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "test_list = []\n",
    "submission_file = \"submission.csv\"\n",
    "with open(submission_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['test_id'] + ['is_duplicate'])\n",
    "    for pred in prediction_submission: \n",
    "        writer.writerow([count] + [float(pred)])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
