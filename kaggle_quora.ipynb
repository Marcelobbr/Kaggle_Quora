{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - quora challenge\n",
    "Team: marsamag\n",
    "* Marcelo Barata Ribeiro\n",
    "* Magno Mendes \n",
    "* Sayuri Takeda\n",
    "\n",
    "We divided the script in 2 parts: first try and second try.\n",
    "\n",
    "The first try and second try are separated because they are pretty different. They were mainly used to test samples of the database. \n",
    "\n",
    "The second try section has its own submission part, where we run the full datasets\n",
    "\n",
    "**First try - TF-IDF with Xgboost**\n",
    "* Successfully submitted.\n",
    "* Log loss obtained: 0.35\n",
    "* Used transformations: tf-idf, word match share. Also we rebalanced the database.\n",
    "* Machine Learning: xgboost\n",
    "* This was the best log loss that we achieved. However, we would make the second try, trying to build a script a bit more manually written by yhe team.\n",
    "\n",
    "**Second try - Using Length features, Fuzzy features, wordvec + other ML**\n",
    "* Successfully submitted, but had errors on tha database.\n",
    "* Log loss obtained: 1.09, because of those errors.\n",
    "* Used transformations: basic (length) features, fuzzywuzzy features, vectors from word2vec. Also we rebalanced the database.\n",
    "* Machine Learning: random forest (best score). Also tested SVC, knn and adaboost.\n",
    "* High log loss. Errors were fixed, but we couldn't save the new fixed database to submit on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from numpy import float32\n",
    "import scipy\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "import pandas\n",
    "import string\n",
    "import warnings\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import threading\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a small sample to test if every function is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = pandas.read_csv(\"/Dados/Kaggle/train.csv\")\n",
    "quora_test = pandas.read_csv(\"/Dados/Kaggle/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename test set column (for correspondence between databases)\n",
    "quora_test = quora_test.rename(index=str, columns={\"test_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Para testar com um sample da base, basta descomentar as 2 linhas abaixo:\n",
    "#quora_train = quora_train[:1000]\n",
    "#quora_test = quora_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to repair errors in the database: missing phrases that became nan on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repair_original_data(data):\n",
    "    for row in data.itertuples():\n",
    "        for question in ['question1', 'question2']:\n",
    "            try: len(data.ix[row[0], question])\n",
    "            except: data.ix[row[0], question] = \"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1985</td>\n",
       "      <td>1986</td>\n",
       "      <td>I am a straight A student but have no motivati...</td>\n",
       "      <td>My fiancée died recently and it pains my heart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "      <td>Which is the best shares to purchase and sale ...</td>\n",
       "      <td>In Sydney, which company would be the best to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>1989</td>\n",
       "      <td>1990</td>\n",
       "      <td>I and my girlfriends private partstouched each...</td>\n",
       "      <td>Why most of the cosmetic products don't have p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1991</td>\n",
       "      <td>1992</td>\n",
       "      <td>Could we use cherenkov atmosphere radiation (w...</td>\n",
       "      <td>Can we map the surface (and the subsurface) of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>What is a good song for lyric prank?</td>\n",
       "      <td>Diving the Blue Hole in Dahab?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  qid1  qid2                                          question1  \\\n",
       "995  995  1985  1986  I am a straight A student but have no motivati...   \n",
       "996  996  1987  1988  Which is the best shares to purchase and sale ...   \n",
       "997  997  1989  1990  I and my girlfriends private partstouched each...   \n",
       "998  998  1991  1992  Could we use cherenkov atmosphere radiation (w...   \n",
       "999  999  1993  1994               What is a good song for lyric prank?   \n",
       "\n",
       "                                             question2  is_duplicate  \n",
       "995  My fiancée died recently and it pains my heart...             0  \n",
       "996  In Sydney, which company would be the best to ...             0  \n",
       "997  Why most of the cosmetic products don't have p...             0  \n",
       "998  Can we map the surface (and the subsurface) of...             1  \n",
       "999                     Diving the Blue Hole in Dahab?             0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAEJCAYAAACJ7W5OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaccd3da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = sns.color_palette()\n",
    "is_dup = quora_train['is_duplicate'].value_counts()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=color[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Is Duplicate', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try - TF-IDF with Xgboost\n",
    "The functions below were inspired on a kernel from kaggler competitor Anokas: https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = quora_train\n",
    "df_test = quora_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_qs = pandas.Series(quora_train['question1'].tolist() + quora_train['question2'].tolist()).astype(str)\n",
    "test_qs = pandas.Series(quora_test['question1'].tolist() + quora_test['question2'].tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = numpy.sum(shared_weights) / numpy.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebalancing Data\n",
    "On the kaggle forum, it was discussed that only 17% of the test set had positives (duplicates). Here we try to rebalance the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in list_stopwords:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "train_word_match = quora_train.apply(word_match_share, axis=1, raw=True)\n",
    "tfidf_train_word_match = quora_train.apply(tfidf_word_match_share, axis=1, raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404285    0.857143\n",
       "404286    0.666667\n",
       "404287    0.500000\n",
       "404288    0.000000\n",
       "404289    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_match.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# First we create our training and testing data\n",
    "x_train = pandas.DataFrame()\n",
    "x_test = pandas.DataFrame()\n",
    "x_train['word_match'] = train_word_match\n",
    "x_train['tfidf_word_match'] = tfidf_train_word_match\n",
    "x_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n",
    "x_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "y_train = quora_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "\n",
    "# Oversampling the negative class\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pandas.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pandas.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train = pandas.concat([pos_train, neg_train])\n",
    "y_train = (numpy.zeros(len(pos_train)) + 1).tolist() + numpy.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202450</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175581</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.110683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261865</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94259</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.187747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271928</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.463781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_match  tfidf_word_match\n",
       "202450    0.571429          0.571005\n",
       "175581    0.100000          0.110683\n",
       "261865    0.363636          0.244444\n",
       "94259     0.142857          0.187747\n",
       "271928    0.444444          0.463781"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.683189\tvalid-logloss:0.683238\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.602041\tvalid-logloss:0.602515\n",
      "[20]\ttrain-logloss:0.544863\tvalid-logloss:0.545663\n",
      "[30]\ttrain-logloss:0.503151\tvalid-logloss:0.504194\n",
      "[40]\ttrain-logloss:0.471989\tvalid-logloss:0.473231\n",
      "[50]\ttrain-logloss:0.448341\tvalid-logloss:0.449746\n",
      "[60]\ttrain-logloss:0.430164\tvalid-logloss:0.431706\n",
      "[70]\ttrain-logloss:0.416072\tvalid-logloss:0.417726\n",
      "[80]\ttrain-logloss:0.405012\tvalid-logloss:0.406775\n",
      "[90]\ttrain-logloss:0.396327\tvalid-logloss:0.398177\n",
      "[100]\ttrain-logloss:0.389488\tvalid-logloss:0.391404\n",
      "[110]\ttrain-logloss:0.384071\tvalid-logloss:0.386038\n",
      "[120]\ttrain-logloss:0.379786\tvalid-logloss:0.381792\n",
      "[130]\ttrain-logloss:0.376375\tvalid-logloss:0.378414\n",
      "[140]\ttrain-logloss:0.373661\tvalid-logloss:0.375724\n",
      "[150]\ttrain-logloss:0.371482\tvalid-logloss:0.373567\n",
      "[160]\ttrain-logloss:0.369728\tvalid-logloss:0.371835\n",
      "[170]\ttrain-logloss:0.368327\tvalid-logloss:0.370453\n",
      "[180]\ttrain-logloss:0.367186\tvalid-logloss:0.369324\n",
      "[190]\ttrain-logloss:0.366276\tvalid-logloss:0.368424\n",
      "[200]\ttrain-logloss:0.365539\tvalid-logloss:0.367697\n",
      "[210]\ttrain-logloss:0.364945\tvalid-logloss:0.367108\n",
      "[220]\ttrain-logloss:0.364443\tvalid-logloss:0.366615\n",
      "[230]\ttrain-logloss:0.364033\tvalid-logloss:0.366215\n",
      "[240]\ttrain-logloss:0.363691\tvalid-logloss:0.365883\n",
      "[250]\ttrain-logloss:0.363389\tvalid-logloss:0.3656\n",
      "[260]\ttrain-logloss:0.363117\tvalid-logloss:0.365335\n",
      "[270]\ttrain-logloss:0.362879\tvalid-logloss:0.365108\n",
      "[280]\ttrain-logloss:0.362655\tvalid-logloss:0.364893\n",
      "[290]\ttrain-logloss:0.362455\tvalid-logloss:0.364698\n",
      "[300]\ttrain-logloss:0.362288\tvalid-logloss:0.36454\n",
      "[310]\ttrain-logloss:0.362151\tvalid-logloss:0.364412\n",
      "[320]\ttrain-logloss:0.362029\tvalid-logloss:0.364298\n",
      "[330]\ttrain-logloss:0.361896\tvalid-logloss:0.364174\n",
      "[340]\ttrain-logloss:0.361792\tvalid-logloss:0.364075\n",
      "[350]\ttrain-logloss:0.361691\tvalid-logloss:0.363984\n",
      "[360]\ttrain-logloss:0.361568\tvalid-logloss:0.36387\n",
      "[370]\ttrain-logloss:0.361455\tvalid-logloss:0.363768\n",
      "[380]\ttrain-logloss:0.361353\tvalid-logloss:0.363673\n",
      "[390]\ttrain-logloss:0.361258\tvalid-logloss:0.363586\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pandas.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try - Using Length features, Fuzzy features, wordvec + other ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create functions to process data\n",
    "We used some processing functions made by Guilherme Wang and Lucas Medeiros team, because its functions were well writen compared to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Editing questions with NLTK package\n",
    "\n",
    "def remove_stopwords(phrase,list_stopwords):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all stopwords from a list\n",
    "    :param phrase: String. A phrase.\n",
    "    :param list_stopwords: List. A list of stopwords\n",
    "    :return: The same phrase without stopwords\n",
    "    \"\"\"\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in list_stopwords:\n",
    "            final_phrase.append((word))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "    \n",
    "def remove_punctuation(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all punctuation from it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase without punctuation\n",
    "    \"\"\"\n",
    "    #Check if NA\n",
    "    if type(phrase) is float:\n",
    "        if math.isnan(phrase):\n",
    "            return (\"\")\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    phrase = phrase.translate(translator) #removing punctuation\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "def lemm_wordnet(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes lemmatizes it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lemmas\n",
    "    \"\"\"\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    #NA is a float type, so this if is to avoid conflict\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    else:\n",
    "        return \"\"\n",
    "    return phrase\n",
    "    \n",
    "def remove_duplicate(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all duplicate words\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase with just unique words\n",
    "    \"\"\"\n",
    "    aux_phrase = []\n",
    "        \n",
    "    if type(phrase) is not float:\n",
    "        \n",
    "        for i in phrase.split():\n",
    "            \n",
    "            if i not in aux_phrase:\n",
    "                aux_phrase.append(i)\n",
    "    \n",
    "    phrase = ' '.join(aux_phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "    \n",
    "def all_lower_case(phrase):    \n",
    "    \"\"\"\n",
    "    Receives a phrase and makes it lower case\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lower case\n",
    "    \"\"\"\n",
    "    if type(phrase) is not float:\n",
    "            phrase = phrase.lower()\n",
    "    return phrase\n",
    "    \n",
    "def stem_snowball(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and returns the same phrase stemmed, lowercase phrase without stopwords\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: String. Stemmed, lowercase phrase without stopwords\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #Stem words according to stemmer\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        final_phrase.append((stemmer.stem(word)))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning tool is used so you can easily choose which functions you want to use to clean te text\n",
    "def cleaning_tool(data, drop_na = True, lower_case = True, rm_duplicate = True, stopwords = True, \n",
    "                  punctuation = True, lemm = False, stem = False):\n",
    "    \"\"\"\n",
    "    Function to process all data using calling functions from above, according to what was chosen.\n",
    "    :param data: data frame.\n",
    "    :param drop_na: If True drop all lines of data frame with NA\n",
    "    :param lower_case: If True transform for lower case\n",
    "    :param rm_duplicate: If True remove all duplicate words in questions\n",
    "    :param stopwords: If True removes stopwords\n",
    "    :param punctuation: If True removes punctuation\n",
    "    :param lemm: If True returns the phrase lemmatized\n",
    "    :param stem: If True returns the phrase stemmed\n",
    "    :param list_of_stopwords: List of stopwords to be used\n",
    "    :return: Question1 and Question2 processed according to parameters\n",
    "    \"\"\"\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)\n",
    "    \n",
    "    if rm_duplicate == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_duplicate(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_duplicate(x))\n",
    "    \n",
    "    if lower_case == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: all_lower_case(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: all_lower_case(x))\n",
    "    \n",
    "    if stopwords == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_stopwords(x, list_stopwords))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_stopwords(x, list_stopwords))\n",
    "       \n",
    "    if punctuation == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_punctuation(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_punctuation(x))\n",
    "        \n",
    "    if lemm_wordnet == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: lemm_wordnet(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: lemm_wordnet(x))\n",
    "        \n",
    "    if stem_snowball == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: stem_snowball(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: stem_snowball(x))\n",
    "    \n",
    "    #We used it two times if some function create a new NA.\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create length and fuzzywuzzy functions\n",
    "Here we create some features for the data, such as phrase length, word length and fuzzyWuzzy features which use Levenstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_basic_features(data):\n",
    "    data[\"len_q1\"] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data[\"len_q2\"] = data.question2.apply(lambda x: len(str(x)))\n",
    "    data[\"diff_len\"] = data.len_q1 - data.len_q2\n",
    "    data[\"len_char_q1\"] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data[\"len_char_q2\"] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data[\"len_word_q1\"] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data[\"len_word_q2\"] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "    data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    return data\n",
    "\n",
    "def make_fuzz_features(data):\n",
    "    data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_partial_token _set_ratio'] = data.apply(lambda x : fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz. partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data ['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1) \n",
    "    data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec  (wikimedia model)\n",
    "Here we create word2vec features and some distance features, such as cossine distance, euclidean distance and Cabberra distance.\n",
    "\n",
    "But we won't use the distance features for the submission, because they were inneffective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_io_files = '/Dados/Word2vec/'\n",
    "wikimedia = os.path.join(path_io_files,'model_wikimedia_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_wikimedia = Word2Vec.load(wikimedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 400    # Word vector dimensionality\n",
    "\n",
    "def makeFeatureVec(words, model,index2word_set, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = numpy.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words.split(\" \"):\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = numpy.add(featureVec,model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = numpy.divide(featureVec,nwords)\n",
    "    #    if all(math.isnan(i) == True\" for i in featureVec): featureVec = numpy.zeros((num_features,),dtype=\"float32\")\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create functions of vector and distance features from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_vectors(df1, df2):\n",
    "    df3 = numpy.add(df1,df2)/2\n",
    "    df_final = numpy.column_stack((df3))\n",
    "    return (df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word_set = set(model_wikimedia.wv.index2word)\n",
    "\n",
    "def make_vector_features(data, question1_vectors, question2_vectors):\n",
    "    for index in range(0,len(data)):\n",
    "        question1_vectors.append(makeFeatureVec(data[\"question1\"].iloc[index],model_wikimedia,index2word_set,num_features))\n",
    "        question2_vectors.append(makeFeatureVec(data[\"question2\"].iloc[index],model_wikimedia,index2word_set,num_features))\n",
    "    question1_vectors = numpy.array(question1_vectors)\n",
    "    question2_vectors = numpy.array(question2_vectors)\n",
    "    \n",
    "    #NANs to 0\n",
    "    where_are_NaNs_1 = numpy.isnan(question1_vectors)\n",
    "    where_are_NaNs_2 = numpy.isnan(question2_vectors)\n",
    "    question1_vectors[where_are_NaNs_1] = 0\n",
    "    question2_vectors[where_are_NaNs_2] = 0\n",
    "\n",
    "    count = 0\n",
    "    empty_array = numpy.empty([1000, 1])\n",
    "        \n",
    "    features_wikimedia = (add_vectors(question1_vectors,question2_vectors))\n",
    "    \n",
    "    wikimedia_df = pandas.DataFrame(features_wikimedia)\n",
    "    wikimedia_df = wikimedia_df.transpose()\n",
    "\n",
    "    #indexing wikimedia_df to correspond to data\n",
    "    wikimedia_df = wikimedia_df.set_index(data.index)\n",
    "    \n",
    "    #merge data and wikimedia_df (which has the word2vec vectors)\n",
    "    data = data.join(wikimedia_df, lsuffix='_left', rsuffix='_right')\n",
    "    \n",
    "    return data\n",
    "    return question1_vectors\n",
    "    return question2_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dist_features(data):\n",
    "    data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    \n",
    "    data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(numpy.nan_to_num(vectors1),\n",
    "                                                              numpy.nan_to_num(vectors2))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create deletion and error checking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to delete features that are irrelevant for the machine learning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_features(data):\n",
    "    del data['question1']\n",
    "    del data['question2']\n",
    "    try: \n",
    "        del data['qid1']\n",
    "        del data['qid2']\n",
    "    except: pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to repair NaNs and check existing variables\n",
    "def repair_cells(data):\n",
    "    variables_list = list(quora_train.columns.values)\n",
    "    print('LIST OF VARIABLES:')\n",
    "    print(variables_list)\n",
    "    for var in variables_list: quora_train[var].fillna(quora_train[var].dropna().median(), inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform database and create features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1985</td>\n",
       "      <td>1986</td>\n",
       "      <td>I am a straight A student but have no motivati...</td>\n",
       "      <td>My fiancée died recently and it pains my heart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "      <td>Which is the best shares to purchase and sale ...</td>\n",
       "      <td>In Sydney, which company would be the best to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>1989</td>\n",
       "      <td>1990</td>\n",
       "      <td>I and my girlfriends private partstouched each...</td>\n",
       "      <td>Why most of the cosmetic products don't have p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1991</td>\n",
       "      <td>1992</td>\n",
       "      <td>Could we use cherenkov atmosphere radiation (w...</td>\n",
       "      <td>Can we map the surface (and the subsurface) of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>What is a good song for lyric prank?</td>\n",
       "      <td>Diving the Blue Hole in Dahab?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  qid1  qid2                                          question1  \\\n",
       "995  995  1985  1986  I am a straight A student but have no motivati...   \n",
       "996  996  1987  1988  Which is the best shares to purchase and sale ...   \n",
       "997  997  1989  1990  I and my girlfriends private partstouched each...   \n",
       "998  998  1991  1992  Could we use cherenkov atmosphere radiation (w...   \n",
       "999  999  1993  1994               What is a good song for lyric prank?   \n",
       "\n",
       "                                             question2  is_duplicate  \n",
       "995  My fiancée died recently and it pains my heart...             0  \n",
       "996  In Sydney, which company would be the best to ...             0  \n",
       "997  Why most of the cosmetic products don't have p...             0  \n",
       "998  Can we map the surface (and the subsurface) of...             1  \n",
       "999                     Diving the Blue Hole in Dahab?             0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "quora_train = repair_original_data(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean text (example: remove stopwords)\n",
    "quora_train = cleaning_tool(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = make_basic_features(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = make_fuzz_features(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "vectors1 = []\n",
    "vectors2 = []\n",
    "quora_train = make_vector_features(quora_train, vectors1, vectors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:616: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.double(np.bitwise_or(u != 0, v != 0).sum()))\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:810: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return abs(u - v).sum() / abs(u + v).sum()\n"
     ]
    }
   ],
   "source": [
    "quora_train = make_dist_features(quora_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete text and id features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = delete_features(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIST OF VARIABLES:\n",
      "['id', 'is_duplicate', 'len_q1', 'len_q2', 'diff_len', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2', 'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token _set_ratio', 'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 'cosine_distance', 'cityblock_distance', 'jaccard_distance', 'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance']\n"
     ]
    }
   ],
   "source": [
    "# Repair for NaNs. Also check all variables.\n",
    "quora_train = repair_cells(quora_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>...</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011629</td>\n",
       "      <td>-0.006707</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.577897</td>\n",
       "      <td>7.460717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.527018</td>\n",
       "      <td>0.468340</td>\n",
       "      <td>0.200937</td>\n",
       "      <td>0.629236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>62</td>\n",
       "      <td>-23</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>-0.020374</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.341662</td>\n",
       "      <td>6.545321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>214.269117</td>\n",
       "      <td>0.414440</td>\n",
       "      <td>0.178581</td>\n",
       "      <td>0.475418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>-0.039440</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>12.286306</td>\n",
       "      <td>1.0</td>\n",
       "      <td>277.804333</td>\n",
       "      <td>0.772225</td>\n",
       "      <td>0.332647</td>\n",
       "      <td>0.929195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>96</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>-0.001407</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>0.084597</td>\n",
       "      <td>3.174992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.566365</td>\n",
       "      <td>0.198093</td>\n",
       "      <td>0.085724</td>\n",
       "      <td>0.216241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014252</td>\n",
       "      <td>-0.072535</td>\n",
       "      <td>0.014478</td>\n",
       "      <td>0.906442</td>\n",
       "      <td>13.165277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>283.028998</td>\n",
       "      <td>0.832564</td>\n",
       "      <td>0.361629</td>\n",
       "      <td>0.938049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 424 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  is_duplicate  len_q1  len_q2  diff_len  len_char_q1  len_char_q2  \\\n",
       "995  995             0      72      49        23           19           16   \n",
       "996  996             0      39      62       -23           16           20   \n",
       "997  997             0      58      33        25           19           14   \n",
       "998  998             1     109      96        13           22           20   \n",
       "999  999             0      21      22        -1           13           12   \n",
       "\n",
       "     len_word_q1  len_word_q2  common_words         ...                397  \\\n",
       "995           10            8             1         ...           0.011629   \n",
       "996            6            9             1         ...          -0.009888   \n",
       "997            6            5             0         ...           0.010313   \n",
       "998           15           14             7         ...           0.033258   \n",
       "999            4            4             0         ...          -0.014252   \n",
       "\n",
       "          398       399  cosine_distance  cityblock_distance  \\\n",
       "995 -0.006707  0.004582         0.577897            7.460717   \n",
       "996 -0.020374  0.009137         0.341662            6.545321   \n",
       "997 -0.039440  0.012401         0.920426           12.286306   \n",
       "998 -0.001407  0.017274         0.084597            3.174992   \n",
       "999 -0.072535  0.014478         0.906442           13.165277   \n",
       "\n",
       "     jaccard_distance  canberra_distance  euclidean_distance  \\\n",
       "995               1.0         241.527018            0.468340   \n",
       "996               1.0         214.269117            0.414440   \n",
       "997               1.0         277.804333            0.772225   \n",
       "998               1.0         143.566365            0.198093   \n",
       "999               1.0         283.028998            0.832564   \n",
       "\n",
       "     minkowski_distance  braycurtis_distance  \n",
       "995            0.200937             0.629236  \n",
       "996            0.178581             0.475418  \n",
       "997            0.332647             0.929195  \n",
       "998            0.085724             0.216241  \n",
       "999            0.361629             0.938049  \n",
       "\n",
       "[5 rows x 424 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_quora_train = quora_train.drop(\"is_duplicate\", axis=1)\n",
    "y_quora_train = quora_train[\"is_duplicate\"]\n",
    "quora_train_features, quora_test_features, quora_train_y, quora_test_y = model_selection.train_test_split(\n",
    "    x_quora_train, y_quora_train, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier(n_estimators=300, max_features='auto', bootstrap=False, \n",
    "                               oob_score=False, n_jobs=-1, random_state=0)\n",
    "randomforest.fit(quora_train_features, quora_train_y)\n",
    "\n",
    "predict = randomforest.predict_proba(quora_test_features)\n",
    "\n",
    "logloss_random_forest = log_loss(quora_test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**knn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(quora_train_features, quora_train_y)\n",
    "\n",
    "knn_predict = knn.predict_proba(quora_test_features)\n",
    "\n",
    "logloss_knn = log_loss(quora_test_y,knn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adaboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboost = AdaBoostClassifier(base_estimator=None,\n",
    "                             n_estimators=200,\n",
    "                             learning_rate=0.1,\n",
    "                             algorithm='SAMME.R',\n",
    "                             random_state=0)\n",
    "aboost.fit(quora_train_features, quora_train_y)\n",
    "\n",
    "aboost_predict = aboost.predict_proba(quora_test_features)\n",
    "\n",
    "logloss_aboost = log_loss(quora_test_y,aboost_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.511135</td>\n",
       "      <td>randomforest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.650239</td>\n",
       "      <td>aboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.751260</td>\n",
       "      <td>knn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Log Loss         Model\n",
       "0  0.511135  randomforest\n",
       "2  0.650239        aboost\n",
       "1  3.751260           knn"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pandas.DataFrame({\n",
    "    'Model': ['randomforest', 'knn', 'aboost'],\n",
    "    'Log Loss': [logloss_random_forest, logloss_knn, logloss_aboost]})\n",
    "models.sort_values(by='Log Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Here we submit our predictions using all the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = pandas.read_csv(\"/Dados/Kaggle/train.csv\")\n",
    "quora_test = pandas.read_csv(\"/Dados/Kaggle/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename database column (for correspondence between databases)\n",
    "quora_test = quora_test.rename(index=str, columns={\"test_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_selected_functions(quora_data):\n",
    "    quora_data = repair_original_data(quora_data)\n",
    "    quora_data = cleaning_tool(quora_data)\n",
    "    quora_data = make_basic_features(quora_data)\n",
    "    quora_data = make_fuzz_features(quora_data)\n",
    "    vectors1 = []\n",
    "    vectors2 = []\n",
    "    quora_data = make_vector_features(quora_data, vectors1, vectors2)\n",
    "    quora_data = delete_features(quora_data)\n",
    "    return quora_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing databases with paralelism\n",
    "We used the version without paralelism, because it worked better. (commented section)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "paralels_list=[]\n",
    "c_size=math.floor(len(quora_test)/60)\n",
    "def thread_saf(i):\n",
    "    if i==0:\n",
    "        beg=0\n",
    "    else:\n",
    "        beg=(i*c_size)\n",
    "    end=(i+1)*c_size\n",
    "    paralel=quora_test[beg:end]\n",
    "    paralel = use_selected_functions(paralel)\n",
    "    paralels_list.append(paralel)\n",
    "    print('finished' + str(i))\n",
    "    return \n",
    "\n",
    "threads=[]\n",
    "for i in range(60):\n",
    "    t=threading.Thread(target=thread_saf,args=(i,))\n",
    "    threads.append(t)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_test = pandas.concat(paralels_list)\n",
    "quora_test = quora_test.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing databases WITHOUT paralelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "quora_train = use_selected_functions(quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "quora_test = use_selected_functions(quora_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_quora_train = quora_train.drop(\"is_duplicate\", axis=1)\n",
    "y_quora_train = quora_train[\"is_duplicate\"]\n",
    "x_quora_test = quora_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save pickle with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train_model_file = 'quora_train_model.pkl'\n",
    "quora_test_model_file = 'quora_test_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(quora_train, open(quora_train_model_file, 'wb'))\n",
    "pickle.dump(quora_test, open(quora_test_model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_train = pickle.load(open(quora_train_model_file, 'rb'))\n",
    "quora_test = pickle.load(open(quora_test_model_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split x_quora_train and y_quora_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_quora_train = quora_train.drop(\"is_duplicate\", axis=1)\n",
    "y_quora_train = quora_train[\"is_duplicate\"]\n",
    "x_quora_test = quora_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=300, n_jobs=-1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest = RandomForestClassifier(n_estimators=300, max_features='auto', bootstrap=False, \n",
    "                               oob_score=False, n_jobs=-1, random_state=0)\n",
    "randomforest.fit(x_quora_train, y_quora_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = randomforest.predict_proba(x_quora_test)\n",
    "prediction_submission = [i[1] for i in predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save pickle with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list_file = 'predictions_list.pkl'\n",
    "pickle.dump(prediction_submission, open(predictions_list_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighting predictions\n",
    "Forum discussions showed that the distribution is imbalaced, with only 20% of the pairs being duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37689944444444451"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mean=numpy.mean(pred_submission)\n",
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_pred_submission = []\n",
    "for pred in pred_submission:\n",
    "    weighted_pred=pred*(0.2/pred_mean)\n",
    "    weighted_pred_submission.append(weighted_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(weighed_pred_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create csv file with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "test_list = []\n",
    "submission_file = \"submission.csv\"\n",
    "with open(submission_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['test_id'] + ['is_duplicate'])\n",
    "    for pred in weighted_pred_submission: \n",
    "        writer.writerow([count] + [float(pred)])\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
